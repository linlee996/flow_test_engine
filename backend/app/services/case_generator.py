"""LangGraph ç”¨ä¾‹ç”Ÿæˆå·¥ä½œæµ - å«äººæœºæ¾„æ¸…å¾ªç¯"""
import uuid
from pathlib import Path
from typing import TypedDict, Annotated, Optional
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import interrupt, Command
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.language_models import BaseChatModel
from app.core.config import get_settings
from app.services.document_parser import ParsedDocument

settings = get_settings()

# åŠ è½½ backend/config/prompt.md ä½œä¸ºç³»ç»Ÿ prompt
PROMPT_FILE = Path(__file__).parent.parent.parent / "config" / "prompt.md"


def load_system_prompt() -> str:
    """åŠ è½½ backend/config/prompt.md ä½œä¸ºç³»ç»Ÿ prompt"""
    if PROMPT_FILE.exists():
        return PROMPT_FILE.read_text(encoding="utf-8")
    raise FileNotFoundError(f"Prompt file not found: {PROMPT_FILE}")


def create_llm(provider: str, api_key: str, base_url: str, model: str) -> BaseChatModel:
    """
    æ ¹æ®å‚å•†åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹
    
    - OpenAI/DeepSeek/Kimi: ä½¿ç”¨ ChatOpenAIï¼ˆOpenAI å…¼å®¹ APIï¼‰
    - Gemini: ä½¿ç”¨ ChatGoogleGenerativeAI
    - Anthropic: ä½¿ç”¨ ChatAnthropic
    """
    provider = provider.lower() if provider else "openai"
    
    # OpenAI å…¼å®¹çš„å‚å•†ï¼ˆOpenAIã€DeepSeekã€Kimiï¼‰
    if provider in ("openai", "deepseek", "kimi"):
        return ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=f"{base_url.rstrip('/')}/v1" if base_url else None,
        )
    
    # Gemini - ä½¿ç”¨ Google çš„ LangChain é›†æˆ
    if provider == "gemini":
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            return ChatGoogleGenerativeAI(
                model=model,
                google_api_key=api_key,
            )
        except ImportError:
            # å¦‚æœæ²¡æœ‰å®‰è£… langchain-google-genaiï¼Œå›é€€åˆ° OpenAI å…¼å®¹æ¨¡å¼
            return ChatOpenAI(
                model=model,
                api_key=api_key,
                base_url=f"{base_url.rstrip('/')}/v1beta/openai",
            )
    
    # Anthropic
    if provider == "anthropic":
        try:
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(
                model=model,
                api_key=api_key,
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£… langchain-anthropic: pip install langchain-anthropic")
    
    # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹
    return ChatOpenAI(
        model=model,
        api_key=api_key,
        base_url=f"{base_url.rstrip('/')}/v1" if base_url else None,
    )


class WorkflowState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€"""
    messages: Annotated[list, add_messages]
    document_content: str  # è§£æåçš„æ–‡æ¡£ markdown
    images: list[dict]  # å¤šæ¨¡æ€å›¾ç‰‡ [{"caption": str, "base64": str}]
    tables: list[dict]  # è¡¨æ ¼å†…å®¹
    system_prompt: str  # ç³»ç»Ÿ promptï¼ˆæ¥è‡ª config/prompt.mdï¼Œå¯è¢«æ¨¡æ¿æ›¿æ¢ï¼‰
    current_step: str  # å½“å‰æ‰§è¡Œæ­¥éª¤
    has_clarification: bool  # æ˜¯å¦æœ‰å¾…æ¾„æ¸…é—®é¢˜
    clarification_questions: str  # å¾…æ¾„æ¸…é—®é¢˜
    report_markdown: str  # æœ€ç»ˆè¾“å‡ºçš„ markdown æŠ¥å‘Š
    is_stopped: bool  # ç”¨æˆ·æ˜¯å¦åœæ­¢ç”Ÿæˆ


class CaseGeneratorWorkflow:
    """ç”¨ä¾‹ç”Ÿæˆå·¥ä½œæµ"""

    def __init__(self, api_key: str, base_url: str, model: str, provider: str = "openai"):
        self.llm = create_llm(
            provider=provider,
            api_key=api_key,
            base_url=base_url,
            model=model,
        )
        self.checkpointer = InMemorySaver()
        self.graph = self._build_graph()
        self.base_prompt = load_system_prompt()

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph å·¥ä½œæµ"""
        builder = StateGraph(WorkflowState)

        builder.add_node("analyze_document", self._analyze_document)
        builder.add_node("human_clarification", self._human_clarification)
        builder.add_node("generate_cases", self._generate_cases)

        builder.add_edge(START, "analyze_document")

        builder.add_conditional_edges(
            "analyze_document",
            self._route_after_analysis,
            {"clarify": "human_clarification", "generate": "generate_cases", "end": END}
        )

        builder.add_conditional_edges(
            "human_clarification",
            self._route_after_clarification,
            {"generate": "generate_cases", "analyze": "analyze_document", "end": END}
        )

        builder.add_edge("generate_cases", END)

        return builder.compile(checkpointer=self.checkpointer)

    @staticmethod
    def _build_multimodal_content(state: WorkflowState) -> list:
        """æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯å†…å®¹ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰"""
        content = []

        # æ·»åŠ æ–‡æ¡£ markdown å†…å®¹
        doc_text = f"## éœ€æ±‚æ–‡æ¡£å†…å®¹\n\n{state['document_content']}"

        # æ·»åŠ è¡¨æ ¼ä¿¡æ¯
        if state.get("tables"):
            doc_text += "\n\n## æ–‡æ¡£ä¸­çš„è¡¨æ ¼\n"
            for t in state["tables"]:
                doc_text += f"\n### {t['caption']}\n{t['markdown']}\n"

        content.append({"type": "text", "text": doc_text})

        # æ·»åŠ å›¾ç‰‡ä½œä¸ºå¤šæ¨¡æ€è¾“å…¥
        images = state.get("images", [])
        if images:
            content.append({"type": "text", "text": "\n\n## æ–‡æ¡£ä¸­çš„å›¾ç‰‡\n"})
            for img in images:
                if img.get("base64"):
                    content.append({"type": "text", "text": f"\n### {img.get('caption', 'å›¾ç‰‡')}\n"})
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img['base64']}"}
                    })

        return content

    def _analyze_document(self, state: WorkflowState) -> dict:
        """åˆ†ææ–‡æ¡£å¹¶ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹è¾“å‡º"""
        system_prompt = state["system_prompt"]

        # æ„å»ºå®Œæ•´æ‰§è¡ŒæŒ‡ä»¤
        analysis_instruction = """è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­çš„å·¥ä½œæµç¨‹ï¼Œå®Œæ•´æ‰§è¡Œæ‰€æœ‰æ­¥éª¤ï¼Œç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹æŠ¥å‘Šã€‚

å¦‚æœåœ¨åˆ†æè¿‡ç¨‹ä¸­å‘ç°éœ€è¦æ¾„æ¸…çš„é—®é¢˜ï¼Œè¯·åœ¨æŠ¥å‘Šä¸­æ˜ç¡®æ ‡æ³¨"æ— æ³•ç»§ç»­ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜éœ€è¦æ¾„æ¸…"ï¼Œå¹¶åˆ—å‡ºå¾…æ¾„æ¸…é—®é¢˜ã€‚"""

        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
        multimodal_content = self._build_multimodal_content(state)
        multimodal_content.append({"type": "text", "text": f"\n\n{analysis_instruction}"})

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=multimodal_content),
        ]

        response = self.llm.invoke(messages)
        response_text = response.content

        # åˆ¤æ–­æ˜¯å¦æœ‰å¾…æ¾„æ¸…é—®é¢˜
        clarification_marker = "æ— æ³•ç»§ç»­ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜éœ€è¦æ¾„æ¸…"
        clarification_questions = ""
        has_clarification = False

        if clarification_marker in response_text:
            marker_pos = response_text.find(clarification_marker)
            clarification_questions = response_text[marker_pos:].strip()
            has_clarification = True

        return {
            "messages": [response],
            "report_markdown": response_text,
            "has_clarification": has_clarification,
            "clarification_questions": clarification_questions,
            "current_step": "analysis_complete",
        }

    @staticmethod
    def _human_clarification(state: WorkflowState) -> dict:
        """äººæœºæ¾„æ¸…èŠ‚ç‚¹ - ç­‰å¾…ç”¨æˆ·è¾“å…¥"""
        user_input = interrupt({
            "type": "clarification_needed",
            "questions": state["clarification_questions"],
            "message": "è¯·æä¾›æ¾„æ¸…ä¿¡æ¯ï¼Œæˆ–è¾“å…¥'å¿½ç•¥å¾…æ¾„æ¸…å†…å®¹ï¼Œç»§ç»­ç”Ÿæˆ'è·³è¿‡ï¼Œæˆ–è¾“å…¥'åœæ­¢ç”Ÿæˆ'ç»ˆæ­¢",
        })

        clarification_text = user_input.get("clarification_input", "")

        if clarification_text == "åœæ­¢ç”Ÿæˆ":
            return {"is_stopped": True}

        if clarification_text == "å¿½ç•¥å¾…æ¾„æ¸…å†…å®¹ï¼Œç»§ç»­ç”Ÿæˆ":
            return {
                "has_clarification": False,
                "messages": [HumanMessage(content="ç”¨æˆ·é€‰æ‹©å¿½ç•¥æ¾„æ¸…é—®é¢˜ï¼Œç»§ç»­ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")],
            }

        return {
            "messages": [HumanMessage(content=f"ç”¨æˆ·æ¾„æ¸…ä¿¡æ¯ï¼š\n{clarification_text}")],
        }

    def _generate_cases(self, state: WorkflowState) -> dict:
        """æ¾„æ¸…åé‡æ–°ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹è¾“å‡º"""
        system_prompt = state["system_prompt"]

        # æ„å»ºå®Œæ•´æ‰§è¡ŒæŒ‡ä»¤ - åŸºäºæ¾„æ¸…ä¿¡æ¯é‡æ–°ç”Ÿæˆ
        generate_instruction = """åŸºäºç”¨æˆ·æä¾›çš„æ¾„æ¸…ä¿¡æ¯ç»§ç»­å®Œæˆåç»­æ­¥éª¤ï¼Œç”Ÿæˆå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹æŠ¥å‘Šã€‚"""

        # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆåŒ…å«æ–‡æ¡£å†…å®¹ï¼‰
        multimodal_content = self._build_multimodal_content(state)
        multimodal_content.append({"type": "text", "text": f"\n\n{generate_instruction}"})

        messages = [SystemMessage(content=system_prompt)]
        messages.extend(state["messages"])
        messages.append(HumanMessage(content=multimodal_content))

        response = self.llm.invoke(messages)

        return {
            "messages": [response],
            "report_markdown": response.content,
            "has_clarification": False,
            "current_step": "generation_complete",
        }

    def _route_after_analysis(self, state: WorkflowState) -> str:
        if state.get("is_stopped"):
            return "end"
        if state.get("has_clarification"):
            return "clarify"
        # æ²¡æœ‰å¾…æ¾„æ¸…é—®é¢˜ï¼Œç›´æ¥ç»“æŸï¼ˆreport_markdown å·²åœ¨ _analyze_document ä¸­ç”Ÿæˆï¼‰
        return "end"

    def _route_after_clarification(self, state: WorkflowState) -> str:
        if state.get("is_stopped"):
            return "end"
        last_msg = state["messages"][-1] if state["messages"] else None
        if last_msg and ("å¾…æ¾„æ¸…é—®é¢˜" in str(last_msg.content) or "ğŸ”´" in str(last_msg.content)):
            return "analyze"
        return "generate"

    def start(self, parsed_doc: ParsedDocument, template_prompt: Optional[str] = None) -> tuple[str, dict]:
        """
        å¯åŠ¨å·¥ä½œæµ
        template_prompt: ç”¨æˆ·è‡ªå®šä¹‰æ¨¡æ¿ï¼Œä¼šæ›¿æ¢ prompt.md ä¸­çš„å†…å®¹
        """
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}

        # ä½¿ç”¨ç”¨æˆ·æ¨¡æ¿æ›¿æ¢ç³»ç»Ÿ promptï¼Œæˆ–ä½¿ç”¨é»˜è®¤ prompt
        system_prompt = template_prompt if template_prompt else self.base_prompt

        initial_state = {
            "messages": [],
            "document_content": parsed_doc.markdown,
            "images": parsed_doc.images,
            "tables": parsed_doc.tables,
            "system_prompt": system_prompt,
            "current_step": "start",
            "has_clarification": False,
            "clarification_questions": "",
            "report_markdown": "",
            "is_stopped": False,
        }

        result = self.graph.invoke(initial_state, config=config)
        return thread_id, result

    def resume(self, thread_id: str, clarification_input: str) -> dict:
        """æ¢å¤å·¥ä½œæµï¼ˆç”¨æˆ·æä¾›æ¾„æ¸…ä¿¡æ¯åï¼‰"""
        config = {"configurable": {"thread_id": thread_id}}
        resume_command = Command(resume={"clarification_input": clarification_input})
        return self.graph.invoke(resume_command, config=config)

    def get_state(self, thread_id: str) -> dict:
        """è·å–å·¥ä½œæµå½“å‰çŠ¶æ€"""
        config = {"configurable": {"thread_id": thread_id}}
        return self.graph.get_state(config)
