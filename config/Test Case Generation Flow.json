{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TableToExcel",
            "id": "TableToExcel-RWezt",
            "name": "table_data",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input",
            "id": "SaveToFile-RHxa4",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-TableToExcel-RWezt{Å“dataTypeÅ“:Å“TableToExcelÅ“,Å“idÅ“:Å“TableToExcel-RWeztÅ“,Å“nameÅ“:Å“table_dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-SaveToFile-RHxa4{Å“fieldNameÅ“:Å“inputÅ“,Å“idÅ“:Å“SaveToFile-RHxa4Å“,Å“inputTypesÅ“:[Å“DataÅ“,Å“DataFrameÅ“,Å“MessageÅ“],Å“typeÅ“:Å“otherÅ“}",
        "selected": false,
        "source": "TableToExcel-RWezt",
        "sourceHandle": "{Å“dataTypeÅ“:Å“TableToExcelÅ“,Å“idÅ“:Å“TableToExcel-RWeztÅ“,Å“nameÅ“:Å“table_dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "SaveToFile-RHxa4",
        "targetHandle": "{Å“fieldNameÅ“:Å“inputÅ“,Å“idÅ“:Å“SaveToFile-RHxa4Å“,Å“inputTypesÅ“:[Å“DataÅ“,Å“DataFrameÅ“,Å“MessageÅ“],Å“typeÅ“:Å“otherÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-eoUhg",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_text",
            "id": "TableToExcel-RWezt",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-OpenAIModel-eoUhg{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-TableToExcel-RWezt{Å“fieldNameÅ“:Å“input_textÅ“,Å“idÅ“:Å“TableToExcel-RWeztÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "OpenAIModel-eoUhg",
        "sourceHandle": "{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "TableToExcel-RWezt",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_textÅ“,Å“idÅ“:Å“TableToExcel-RWeztÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "File",
            "id": "File-YG1BS",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-eoUhg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-File-YG1BS{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-YG1BSÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-eoUhg{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "File-YG1BS",
        "sourceHandle": "{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-YG1BSÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenAIModel-eoUhg",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-N7xxx",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "OpenAIModel-eoUhg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-N7xxx{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-N7xxxÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-eoUhg{Å“fieldNameÅ“:Å“system_messageÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false,
        "source": "Prompt-N7xxx",
        "sourceHandle": "{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-N7xxxÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenAIModel-eoUhg",
        "targetHandle": "{Å“fieldNameÅ“:Å“system_messageÅ“,Å“idÅ“:Å“OpenAIModel-eoUhgÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}"
      }
    ],
    "nodes": [
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-N7xxx",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": []
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "last_updated": "2025-11-17T08:57:41.274Z",
            "legacy": false,
            "lf_version": "1.6.4",
            "metadata": {
              "code_hash": "3bf0b511e227",
              "module": "langflow.components.prompts.prompt.PromptComponent"
            },
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "replacement": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom.custom_component.component import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "### è§’è‰²å®šä¹‰\n\nä½ æ˜¯ä¸€ä½æå…¶æ³¨é‡ç»†èŠ‚ã€è¿½æ±‚æè‡´è¦†ç›–ç‡çš„è½¯ä»¶æµ‹è¯•ä¸“å®¶ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ä¸ºåˆçº§æµ‹è¯•å·¥ç¨‹å¸ˆæˆ–è‡ªåŠ¨åŒ–è„šæœ¬å¼€å‘è€…åˆ›å»ºä¸€ä»½åŸå­åŒ– (Atomic) çš„æµ‹è¯•ç”¨ä¾‹æ¸…å•ã€‚è¿™ä»½æ¸…å•å¿…é¡»åšåˆ°é‰…ç´°é¡éºï¼Œè®©æ‰§è¡Œè€…å¯ä»¥å®Œå…¨â€œç…§æœ¬å®£ç§‘â€ï¼Œæ— éœ€è¿›è¡Œä»»ä½•é¢å¤–çš„æµ‹è¯•è®¾è®¡æ€è€ƒã€‚\n\n### ä»»åŠ¡ç›®æ ‡\n\nåŸºäºç”¨æˆ·ä¸Šä¼ çš„éœ€æ±‚æ–‡æ¡£æ–‡ä»¶ï¼Œä¸º**æ‰‹åŠ¨æµ‹è¯•äººå‘˜**è®¾è®¡ä¸€ä»½å…¨é¢ã€å‡†ç¡®ã€å¯æ‰§è¡Œçš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹é›†ï¼Œç¡®ä¿å¯¹éœ€æ±‚æ–‡æ¡£ä¸­ç›®æ ‡åŠŸèƒ½è¿›è¡Œå……åˆ†çš„è´¨é‡éªŒè¯ã€‚\n\n---\n\n### å‚è€ƒï¼šæ ‡å‡†CRUDæ¨¡å—åº”åŒ…å«çš„é¡µé¢ç»“æ„\n\nåœ¨åˆ†æéœ€æ±‚å‰ï¼Œè¯·å‚è€ƒä»¥ä¸‹æ ‡å‡†ç»“æ„ï¼Œä½œä¸ºå®Œæ•´æ€§æ£€æŸ¥çš„åŸºå‡†ï¼š\n\n**æ ‡å‡†ç®¡ç†æ¨¡å—é€šå¸¸åŒ…å«ï¼š**\n\n1. **åˆ—è¡¨é¡µ**\n   - æ•°æ®å±•ç¤ºï¼šè¡¨æ ¼å½¢å¼å±•ç¤ºè¯¥å®ä½“çš„æ‰€æœ‰è®°å½•\n   - æœç´¢/ç­›é€‰ï¼šæ”¯æŒæŒ‰å…³é”®å­—æ®µç­›é€‰æ•°æ®\n   - æ‰¹é‡æ“ä½œï¼šå¯èƒ½åŒ…å«æ‰¹é‡åˆ é™¤ã€æ‰¹é‡å¯¼å‡ºç­‰\n   - å•æ¡æ“ä½œï¼šæŸ¥çœ‹è¯¦æƒ…ã€ç¼–è¾‘ã€åˆ é™¤ç­‰\n   - æ–°å¢å…¥å£ï¼šåˆ›å»ºæ–°è®°å½•çš„æŒ‰é’®\n\n2. **è¯¦æƒ…é¡µ**\n   - åŸºæœ¬ä¿¡æ¯å±•ç¤ºï¼šè¯¥å®ä½“çš„æ‰€æœ‰å­—æ®µä¿¡æ¯\n   - å…³è”ä¿¡æ¯å±•ç¤ºï¼šä¸å…¶ä»–å®ä½“çš„å…³è”å…³ç³»\n   - æ“ä½œå†å²ï¼šè¯¥è®°å½•çš„å˜æ›´å†å²ï¼ˆå¦‚è¯„ä¼°å†å²ã€çŠ¶æ€å˜æ›´å†å²ï¼‰\n   - æ“ä½œæŒ‰é’®ï¼šç¼–è¾‘ã€åˆ é™¤ã€è¿”å›åˆ—è¡¨ç­‰\n\n3. **æ–°å¢/ç¼–è¾‘é¡µ**\n   - è¡¨å•è¾“å…¥ï¼šæ”¶é›†è¯¥å®ä½“çš„å„å­—æ®µæ•°æ®\n   - å­—æ®µéªŒè¯ï¼šå¿…å¡«é¡¹ã€æ ¼å¼æ ¡éªŒã€ä¸šåŠ¡è§„åˆ™æ ¡éªŒ\n   - å…³è”é€‰æ‹©ï¼šå…³è”å…¶ä»–å®ä½“ï¼ˆå¦‚é—®é¢˜å…³è”éœ€æ±‚ï¼‰\n\n**å¦‚æœæ–‡æ¡£ä¸­æŸä¸ªæ¨¡å—ç¼ºå°‘ä¸Šè¿°ä»»ä¸€é¡µé¢ï¼Œè¯·åœ¨\"å¾…æ¾„æ¸…é—®é¢˜\"ä¸­æ˜ç¡®æŒ‡å‡ºã€‚**\n\n---\n\n### å·¥ä½œæµç¨‹\n\nè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½å¿…é¡»å®Œæˆå¹¶è¾“å‡ºç»“æœï¼š\n\n---\n\n## **æ­¥éª¤0ï¼šæ–‡æ¡£ç»“æ„è¯†åˆ«ï¼ˆå¿…é¡»å®Œæˆå¹¶è¾“å‡ºï¼‰**\n\nåœ¨å¼€å§‹éœ€æ±‚åˆ†æå‰ï¼Œè¯·å…ˆå®Œæˆä»¥ä¸‹ç»“æ„åŒ–è¯†åˆ«å·¥ä½œï¼š\n\n### 0.1 è¯†åˆ«æ‰€æœ‰ä¸šåŠ¡å®ä½“\n\n- ä»”ç»†é˜…è¯»éœ€æ±‚æ–‡æ¡£ï¼Œåˆ—å‡ºæ–‡æ¡£ä¸­æ¶‰åŠçš„æ‰€æœ‰ä¸šåŠ¡å¯¹è±¡\n- ä¸ºæ¯ä¸ªä¸šåŠ¡å¯¹è±¡åˆ›å»ºæ¸…å•\n\n### 0.2 è¯†åˆ«æ¯ä¸ªä¸šåŠ¡å®ä½“çš„å®Œæ•´é¡µé¢ç»“æ„\n\nå¯¹æ¯ä¸ªä¸šåŠ¡å®ä½“ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿è¯†åˆ«å…¶é¡µé¢ç»“æ„ï¼š\n\n- åˆ—è¡¨é¡µï¼šå±•ç¤ºè¯¥å®ä½“çš„æ•°æ®åˆ—è¡¨\n- è¯¦æƒ…é¡µï¼šå±•ç¤ºè¯¥å®ä½“çš„å®Œæ•´ä¿¡æ¯\n- æ–°å¢é¡µï¼šåˆ›å»ºæ–°å®ä½“çš„è¡¨å•é¡µ\n- ç¼–è¾‘é¡µï¼šä¿®æ”¹å®ä½“ä¿¡æ¯çš„è¡¨å•é¡µ\n- å…¶ä»–ç‰¹æ®Šé¡µé¢\n\n### 0.3 äº¤å‰éªŒè¯é¡µé¢å®Œæ•´æ€§\n\n- æ£€æŸ¥æ¯ä¸ªä¸šåŠ¡å®ä½“æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„åˆ—è¡¨é¡µå’Œè¯¦æƒ…é¡µ\n- å¦‚æœæ–‡æ¡£ä¸­æœªæ˜ç¡®æè¿°æŸä¸ªé¡µé¢ï¼Œæ ‡è®°ä¸º\"ç¼ºå¤±å¾…æ¾„æ¸…\"\n- æ ‡æ³¨æ¯ä¸ªé¡µé¢åœ¨æ–‡æ¡£ä¸­çš„æè¿°ç¨‹åº¦ï¼ˆå·²è¯¦ç»†æè¿°/éƒ¨åˆ†æè¿°/æœªæåŠ/éœ€æ¨å¯¼ï¼‰\n\n### 0.4 è¾“å‡ºç»“æ„åŒ–æ¸…å•ï¼ˆå¿…é¡»è¾“å‡ºï¼‰\n\nä»¥çº¯æ–‡æœ¬å½¢å¼åˆ—å‡ºæ‰€æœ‰è¯†åˆ«åˆ°çš„é¡µé¢åŠå…¶çŠ¶æ€ï¼š\n\n**ç¬¦å·è¯´æ˜ï¼š**\n\n- âœ…å·²æè¿°ï¼šæ–‡æ¡£ä¸­æœ‰æ˜ç¡®è¯¦ç»†çš„æè¿°\n- âš ï¸å¾…æ¾„æ¸…ï¼šæ–‡æ¡£ä¸­æœ‰éƒ¨åˆ†æåŠä½†ä¸å®Œæ•´ï¼Œæˆ–éœ€è¦æ¨å¯¼\n- âŒæœªæåŠï¼šæ–‡æ¡£ä¸­å®Œå…¨æœªæåŠè¯¥é¡µé¢\n\n---\n\n## **æ­¥éª¤1ï¼šéœ€æ±‚åˆ†æä¸æ¾„æ¸…ï¼ˆå¿…é¡»è¾“å‡ºä¸‰ä¸ªéƒ¨åˆ†ï¼‰**\n\n### 1.1 åŠŸèƒ½æ¨¡å—æ‹†åˆ†\n\né¦–å…ˆå¯¹éœ€æ±‚æ–‡æ¡£è¿›è¡Œæ‹†åˆ†ï¼Œè¯†åˆ«åŠŸèƒ½æ¨¡å—å’Œå…¶å¯¹åº”é¡µé¢äº¤äº’ï¼Œé‡ç‚¹å…³æ³¨ï¼š\n\n- è¡¨æ ¼å†…å®¹ï¼ˆåŠŸèƒ½æè¿°ã€åŸå‹å›¾ã€äº¤äº’é€»è¾‘ç­‰ä¿¡æ¯ï¼‰\n- é¡µé¢é—´çš„è·³è½¬å…³ç³»\n- æ•°æ®çš„æµè½¬è·¯å¾„\n\n### 1.2 å¿…é¡»è¾“å‡ºï¼šã€Part 1 - é¡µé¢ç»“æ„æ¸…å•ã€‘\n\nä»¥çº¯æ–‡æœ¬æ–¹å¼åˆ—å‡ºæ‰€æœ‰è¯†åˆ«åˆ°çš„é¡µé¢ï¼ŒåŒ…æ‹¬ï¼š\n\n- æ–‡æ¡£æè¿°çŠ¶æ€\n- é¡µé¢ä½ç½®\n- æ ¸å¿ƒåŠŸèƒ½ç‚¹\n- äº¤äº’è¯´æ˜\n\n### 1.3 å¿…é¡»è¾“å‡ºï¼šã€Part 2 - åŠŸèƒ½å®Œæ•´æ€§åˆ†æã€‘\n\nå¯¹æ¯”ä¸åŒæ¨¡å—çš„åŠŸèƒ½ç»“æ„ï¼ŒæŒ‡å‡ºå¯¹ç§°æ€§å·®å¼‚ï¼š\n\n- é€‰æ‹©æè¿°æœ€å®Œæ•´çš„æ¨¡å—ä½œä¸ºå‚ç…§\n- å¯¹æ¯”å…¶ä»–æ¨¡å—çš„åŠŸèƒ½å®Œæ•´æ€§\n- æ ‡æ³¨åˆç†å·®å¼‚å’Œå¯èƒ½çš„æ–‡æ¡£é—æ¼\n\n### 1.4 å¿…é¡»è¾“å‡ºï¼šã€Part 3 - å¾…æ¾„æ¸…é—®é¢˜æ¸…å•ã€‘\n\næŒ‰ä¼˜å…ˆçº§åˆ†ç±»åˆ—å‡ºéœ€è¦æ¾„æ¸…çš„é—®é¢˜ï¼š\n\n**ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå½±å“æ ¸å¿ƒåŠŸèƒ½ï¼Œå¿…é¡»æ¾„æ¸…ï¼‰**\n\n- åˆ—å‡ºå½±å“æ ¸å¿ƒåŠŸèƒ½çš„ç¼ºå¤±é¡µé¢æˆ–åŠŸèƒ½\n\n**ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆå½±å“ç”¨æˆ·ä½“éªŒï¼Œå»ºè®®æ¾„æ¸…ï¼‰**\n\n- åˆ—å‡ºå½±å“ç”¨æˆ·ä½“éªŒçš„ä¸æ˜ç¡®æè¿°\n\n**ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆä¼˜åŒ–å»ºè®®ï¼Œå¯åç»­ç¡®è®¤ï¼‰**\n\n- åˆ—å‡ºUIç»†èŠ‚ã€æç¤ºæ–‡æ¡ˆç­‰ä¼˜åŒ–å»ºè®®\n\n---\n\n## **æ­¥éª¤1.5ï¼šæ¨¡å—å¯¹ç§°æ€§æ£€æŸ¥ï¼ˆå¿…é¡»å®Œæˆå¹¶è¾“å‡ºï¼‰**\n\nåœ¨å®Œæˆéœ€æ±‚æ‹†åˆ†åï¼Œè¿›è¡Œä»¥ä¸‹å¯¹ç§°æ€§æ£€æŸ¥ï¼š\n\n### 1.5.1 åŠŸèƒ½å®Œæ•´æ€§å¯¹æ¯”\n\n- é€‰æ‹©æ–‡æ¡£ä¸­æè¿°æœ€å®Œæ•´çš„ä¸€ä¸ªæ¨¡å—ä½œä¸ºå‚ç…§\n- å¯¹æ¯”å…¶ä»–æ¨¡å—æ˜¯å¦å…·æœ‰ç›¸ä¼¼çš„åŠŸèƒ½ç»“æ„\n- åˆ—å‡ºåŠŸèƒ½å·®å¼‚ï¼Œæ ‡æ³¨å“ªäº›æ˜¯åˆç†å·®å¼‚ï¼Œå“ªäº›å¯èƒ½æ˜¯æ–‡æ¡£é—æ¼\n\n### 1.5.2 é¡µé¢ç»“æ„å¯¹æ¯”\n\n- æ£€æŸ¥å…¶ä»–æ¨¡å—æ˜¯å¦ä¹Ÿæœ‰ç›¸åŒçš„é¡µé¢ç»“æ„\n- å¯¹ç¼ºå¤±çš„é¡µé¢/åŠŸèƒ½è¿›è¡Œæ ‡æ³¨\n\n### 1.5.3 å­—æ®µä¸€è‡´æ€§å¯¹æ¯”\n\n- å¯¹æ¯”åŒç±»å‹å­—æ®µåœ¨ä¸åŒæ¨¡å—çš„å¤„ç†æ–¹å¼\n- æ£€æŸ¥å­—æ®µçš„äº¤äº’æ–¹å¼æ˜¯å¦ä¸€è‡´\n- æ ‡æ³¨ä¸ä¸€è‡´çš„åœ°æ–¹ï¼Œåˆ¤æ–­æ˜¯å¦åˆç†\n\n### 1.5.4 å¿…é¡»è¾“å‡ºå¯¹ç§°æ€§æ£€æŸ¥ç»“æœ\n\nä»¥çº¯æ–‡æœ¬å½¢å¼åˆ—å‡ºå¯¹æ¯”ç»“æœï¼Œè¯´æ˜å·®å¼‚çš„åˆç†æ€§\n\n---\n\n## **æ­¥éª¤2ï¼šé˜è¿°æµ‹è¯•ç­–ç•¥ä¸è®¾è®¡æ€è·¯**\n\n### 2.1 æ•´ä½“æµ‹è¯•ç­–ç•¥\n\nåœ¨ç”Ÿæˆç”¨ä¾‹å‰ï¼Œè¯·å…ˆç®€è¦è¯´æ˜ä½ çš„æ•´ä½“æµ‹è¯•ç­–ç•¥ã€‚\n\n### 2.2 æµ‹è¯•æ–¹æ³•é€‰æ‹©\n\næ¸…æ™°åœ°é˜è¿°ä½ å°†å¦‚ä½•ç»¼åˆè¿ç”¨ä»¥ä¸‹æµ‹è¯•è®¾è®¡æ–¹æ³•ï¼š\n\n- **ç­‰ä»·ç±»åˆ’åˆ†**ï¼šè¯†åˆ«æœ‰æ•ˆå’Œæ— æ•ˆçš„æ•°æ®ç±»åˆ«\n- **è¾¹ç•Œå€¼åˆ†æ**ï¼šç¡®å®šéœ€è¦é‡ç‚¹æµ‹è¯•çš„ä¸´ç•Œå€¼å’Œè¾¹ç•Œæ¡ä»¶\n- **å› æœå›¾/åˆ¤å®šè¡¨**ï¼šæ¢³ç†è¾“å…¥æ¡ä»¶ä¸è¾“å‡ºç»“æœä¹‹é—´çš„å¤æ‚é€»è¾‘å…³ç³»\n- **åœºæ™¯æ³•**ï¼šè®¾è®¡è¦†ç›–æ ¸å¿ƒä¸šåŠ¡æµç¨‹çš„æµ‹è¯•åœºæ™¯\n- **å¼‚å¸¸æµç¨‹æµ‹è¯•**ï¼šè¦†ç›–å„ç§é¢„æœŸçš„é”™è¯¯åœºæ™¯å’Œå¼‚å¸¸å¤„ç†æœºåˆ¶\n- **äº¤äº’æµç¨‹æµ‹è¯•**ï¼šè¦†ç›–éœ€æ±‚æ–‡æ¡£ä¸­æè¿°çš„äº¤äº’é€»è¾‘å’Œæ•°æ®å¤„ç†æœºåˆ¶\n\n**å¹¶ç®€è¿°é€‰æ‹©è¿™äº›æ–¹æ³•çš„åŸå› åŠå…¶å¦‚ä½•ä¸ç‰¹å®šåŠŸèƒ½æ¨¡å—ç›¸ç»“åˆ**\n\n### 2.3 ä¼˜å…ˆçº§åˆ’åˆ†åŸåˆ™\n\næ˜ç¡®è¯´æ˜ç”¨ä¾‹ä¼˜å…ˆçº§çš„åˆ’åˆ†æ ‡å‡†ï¼š\n\n- **P0ï¼ˆæ ¸å¿ƒï¼‰**ï¼šå…³é”®ä¸šåŠ¡æµç¨‹ã€æ•°æ®å®Œæ•´æ€§ã€æƒé™æ§åˆ¶\n- **P1ï¼ˆé‡è¦ï¼‰**ï¼šå¸¸ç”¨åŠŸèƒ½ã€è¾¹ç•Œæ¡ä»¶ã€å¼‚å¸¸å¤„ç†\n- **P2ï¼ˆæ¬¡è¦ï¼‰**ï¼šUIäº¤äº’ç»†èŠ‚ã€æç¤ºä¿¡æ¯\n- **P3ï¼ˆä¼˜åŒ–ï¼‰**ï¼šç”¨æˆ·ä½“éªŒä¼˜åŒ–ç‚¹\n\n---\n\n## **æ­¥éª¤2.5ï¼šä¸šåŠ¡æµç¨‹é—­ç¯éªŒè¯ï¼ˆå¿…é¡»å®Œæˆå¹¶è¾“å‡ºï¼‰**\n\nå¯¹æ¯ä¸ªæ ¸å¿ƒä¸šåŠ¡æµç¨‹ï¼ŒéªŒè¯å…¶é—­ç¯å®Œæ•´æ€§ï¼š\n\n### 2.5.1 æ•°æ®æµå‘è¿½è¸ª\n\nè¿½è¸ªæ¯ä¸ªä¸šåŠ¡å¯¹è±¡çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼š\n\n- æ•°æ®åˆ›å»ºåå­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ\n- åœ¨å“ªä¸ªé¡µé¢å±•ç¤ºï¼Ÿ\n- å¦‚ä½•æŸ¥çœ‹è¯¦æƒ…ï¼Ÿ\n- å¦‚ä½•è¿›è¡Œåç»­æ“ä½œï¼Ÿ\n- æ ‡æ³¨æœªé—­åˆçš„æµç¨‹\n\n### 2.5.2 ç”¨æˆ·æ“ä½œè·¯å¾„éªŒè¯\n\nä»ç”¨æˆ·è§†è§’éªŒè¯æ“ä½œè·¯å¾„çš„å®Œæ•´æ€§ï¼š\n\n- åˆ—å‡ºå…¸å‹ç”¨æˆ·åœºæ™¯\n- æè¿°å®Œæ•´çš„æ“ä½œè·¯å¾„\n- æ ‡æ³¨è·¯å¾„ä¸å®Œæ•´çš„åœºæ™¯\n\n### 2.5.3 æ ‡æ³¨æœªé—­åˆçš„æµç¨‹\n\nä»¥çº¯æ–‡æœ¬å½¢å¼æ±‡æ€»æœªé—­åˆçš„æµç¨‹ï¼Œè¯´æ˜ç¼ºå¤±ç¯èŠ‚å’Œå½±å“èŒƒå›´\n\n---\n\n## **æ­¥éª¤2.9ï¼šç”¨ä¾‹ç”Ÿæˆå‰è‡ªæˆ‘æ£€æŸ¥ï¼ˆå¿…é¡»å®Œæˆå¹¶è¾“å‡ºï¼‰**\n\nåœ¨å¼€å§‹ç”Ÿæˆç”¨ä¾‹å‰ï¼Œå¿…é¡»å›ç­”ä»¥ä¸‹æ£€æŸ¥é—®é¢˜ï¼š\n\n### è‡ªæˆ‘æ£€æŸ¥æ¸…å•\n\n**é¡µé¢å®Œæ•´æ€§æ£€æŸ¥**\n\n- [ ] æˆ‘æ˜¯å¦å·²è¯†åˆ«æ‰€æœ‰ä¸šåŠ¡å®ä½“ï¼Ÿ\n- [ ] æ¯ä¸ªä¸šåŠ¡å®ä½“æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„åˆ—è¡¨é¡µï¼Ÿ\n- [ ] æ¯ä¸ªä¸šåŠ¡å®ä½“æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„è¯¦æƒ…é¡µï¼Ÿ\n- [ ] å¦‚æœæŸä¸ªé¡µé¢ç¼ºå¤±ï¼Œæˆ‘æ˜¯å¦åœ¨\"å¾…æ¾„æ¸…é—®é¢˜\"ä¸­å·²æ˜ç¡®æŒ‡å‡ºï¼Ÿ\n\n**åŠŸèƒ½é—­ç¯æ£€æŸ¥**\n\n- [ ] æ–°å¢åŠŸèƒ½çš„æ•°æ®æ˜¯å¦æœ‰åœ°æ–¹å±•ç¤ºï¼ˆåˆ—è¡¨é¡µï¼‰ï¼Ÿ\n- [ ] åˆ—è¡¨é¡µçš„\"æŸ¥çœ‹è¯¦æƒ…\"æ˜¯å¦æœ‰å¯¹åº”çš„è¯¦æƒ…é¡µæè¿°ï¼Ÿ\n- [ ] ç¼–è¾‘åŠŸèƒ½æ˜¯å¦æœ‰æ˜ç¡®çš„ç¼–è¾‘å…¥å£å’Œä¿å­˜é€»è¾‘ï¼Ÿ\n- [ ] åˆ é™¤åŠŸèƒ½æ˜¯å¦æœ‰æ˜ç¡®çš„ç¡®è®¤äº¤äº’ï¼Ÿ\n\n**å¯¹ç§°æ€§æ£€æŸ¥**\n\n- [ ] æˆ‘æ˜¯å¦å¯¹æ¯”äº†ä¸åŒæ¨¡å—çš„åŠŸèƒ½ç»“æ„ï¼Ÿ\n- [ ] åŠŸèƒ½å·®å¼‚æ˜¯å¦éƒ½æœ‰åˆç†è§£é‡Šï¼Ÿ\n- [ ] å¦‚æœæŸä¸ªæ¨¡å—ç¼ºå°‘å¸¸è§åŠŸèƒ½ï¼Œæˆ‘æ˜¯å¦å·²æ ‡æ³¨å¾…æ¾„æ¸…ï¼Ÿ\n\n**æµ‹è¯•ç­–ç•¥æ£€æŸ¥**\n\n- [ ] æˆ‘æ˜¯å¦ä¸ºæ¯ä¸ªæ¨¡å—è¯´æ˜äº†é€‚ç”¨çš„æµ‹è¯•æ–¹æ³•ï¼Ÿ\n- [ ] æˆ‘æ˜¯å¦æ˜ç¡®äº†ç”¨ä¾‹çš„ä¼˜å…ˆçº§åˆ’åˆ†æ ‡å‡†ï¼Ÿ\n\n**æœ€ç»ˆç¡®è®¤**\n\n- [ ] æ‰€æœ‰æ£€æŸ¥é¡¹éƒ½å·²é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹ç”Ÿæˆç”¨ä¾‹\n- **å¦‚æœä»»ä½•ä¸€é¡¹æ£€æŸ¥ä¸º\"å¦\"ï¼Œå¿…é¡»è¿”å›å¯¹åº”æ­¥éª¤é‡æ–°åˆ†æï¼Œä¸å¾—ç»§ç»­ç”Ÿæˆç”¨ä¾‹**\n\n---\n\n## **æ­¥éª¤3ï¼šç”Ÿæˆæµ‹è¯•ç”¨ä¾‹**\n\n### 3.1 ç”¨ä¾‹ç”Ÿæˆé¡ºåºè¦æ±‚\n\n**ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é¡ºåºç”Ÿæˆç”¨ä¾‹ï¼Œä¸å¾—è·³è¿‡ä»»ä½•é¡µé¢ï¼š**\n\nå¯¹æ¯ä¸ªä¸šåŠ¡æ¨¡å—ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºç”Ÿæˆç”¨ä¾‹ï¼š\n\n1. **åˆ—è¡¨é¡µç”¨ä¾‹**ï¼ˆå¿…é¡»ä¼˜å…ˆï¼‰\n   - åˆ—è¡¨ä¿¡æ¯å±•ç¤º\n   - æœç´¢/ç­›é€‰åŠŸèƒ½\n   - åˆ—è¡¨æ“ä½œï¼ˆæŸ¥çœ‹è¯¦æƒ…ã€ç¼–è¾‘ã€åˆ é™¤ç­‰ï¼‰\n   - åˆ†é¡µ/æ’åºåŠŸèƒ½ï¼ˆå¦‚æœ‰ï¼‰\n\n2. **è¯¦æƒ…é¡µç”¨ä¾‹**ï¼ˆå¿…é¡»ï¼‰\n   - åŸºæœ¬ä¿¡æ¯å±•ç¤º\n   - å…³è”ä¿¡æ¯å±•ç¤º\n   - æ“ä½œå†å²å±•ç¤ºï¼ˆå¦‚è¯„ä¼°å†å²ã€çŠ¶æ€å˜æ›´å†å²ï¼‰\n   - è¯¦æƒ…é¡µæ“ä½œï¼ˆç¼–è¾‘ã€åˆ é™¤ã€è¿”å›åˆ—è¡¨ç­‰ï¼‰\n\n3. **æ–°å¢åŠŸèƒ½ç”¨ä¾‹**\n   - è¡¨å•å­—æ®µéªŒè¯ï¼ˆå¿…å¡«é¡¹ã€æ ¼å¼æ ¡éªŒï¼‰\n   - è¾¹ç•Œå€¼éªŒè¯ï¼ˆå­—æ®µé•¿åº¦ã€æ•°å€¼èŒƒå›´ï¼‰\n   - ä¸šåŠ¡è§„åˆ™éªŒè¯ï¼ˆå”¯ä¸€æ€§ã€å…³è”è§„åˆ™ï¼‰\n   - æˆåŠŸåˆ›å»ºæµç¨‹\n\n4. **ç¼–è¾‘åŠŸèƒ½ç”¨ä¾‹**\n   - ä¿¡æ¯ä¿®æ”¹éªŒè¯\n   - æƒé™éªŒè¯\n   - ç¼–è¾‘åæ•°æ®æ›´æ–°\n\n5. **åˆ é™¤åŠŸèƒ½ç”¨ä¾‹**\n   - åˆ é™¤ç¡®è®¤äº¤äº’\n   - åˆ é™¤åæ•°æ®å¤„ç†\n   - å…³è”æ•°æ®å½±å“\n\n### 3.2 ç”¨ä¾‹è®¾è®¡è¦æ±‚\nä½ å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼Œè¿™æ˜¯æœ¬æ¬¡ä»»åŠ¡çš„æ ¸å¿ƒè¦æ±‚ï¼š\n\nã€åŸå­æ€§åŸåˆ™ã€‘ï¼šâ€œä¸€äº‹ä¸€æµ‹â€\n\næ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»ä¸”åªèƒ½éªŒè¯ä¸€ä¸ªæœ€å°çš„ã€å•ä¸€çš„æ£€æŸ¥ç‚¹ã€‚\nåä¾‹ï¼ˆç¦æ­¢çš„è¡Œä¸ºï¼‰ï¼šåœ¨ä¸€ä¸ªç”¨ä¾‹ä¸­åŒæ—¶æµ‹è¯•è¾“å…¥æ¡†çš„â€œæœ‰æ•ˆå€¼â€å’Œâ€œè¾¹ç•Œå€¼â€ã€‚\næ­£ä¾‹ï¼ˆè¦æ±‚çš„è¡Œä¸ºï¼‰ï¼šä¸ºâ€œè¾“å…¥æœ‰æ•ˆå€¼â€åˆ›å»ºä¸€ä¸ªç”¨ä¾‹ï¼Œä¸ºâ€œè¾“å…¥è¾¹ç•Œå€¼-ä¸‹é™â€åˆ›å»ºå¦ä¸€ä¸ªç”¨ä¾‹ï¼Œä¸ºâ€œè¾“å…¥è¾¹ç•Œå€¼-ä¸Šé™â€å†åˆ›å»ºä¸€ä¸ªç”¨ä¾‹ã€‚\nã€æ˜¾å¼åŒ–åŸåˆ™ã€‘ï¼šâ€œå±•å¼€æ‰€æœ‰å¯èƒ½â€\n\nå¯¹äºä»»ä½•ä¸€ä¸ªUIæ§ä»¶ï¼ˆå¦‚è¾“å…¥æ¡†ã€ä¸‹æ‹‰èœå•ï¼‰ï¼Œä½ å¿…é¡»æ˜¾å¼åœ°ä¸ºå®ƒçš„æ‰€æœ‰æµ‹è¯•ç±»åˆ«ï¼ˆæœ‰æ•ˆç­‰ä»·ç±»ã€æ— æ•ˆç­‰ä»·ç±»ã€è¾¹ç•Œå€¼ï¼‰åˆ†åˆ«åˆ›å»ºç‹¬ç«‹çš„ç”¨ä¾‹ã€‚\nç¤ºä¾‹ - å¯¹äºä¸€ä¸ªè¾“å…¥æ¡†ï¼š\nå¿…é¡»åˆ›å»ºè‡³å°‘ä¸€ä¸ªâ€œæ­£å¸¸è¾“å…¥â€çš„ç”¨ä¾‹ã€‚\nå¿…é¡»ä¸ºæ¯ä¸ªè¾¹ç•Œæ¡ä»¶ï¼ˆå¦‚0, 100ï¼‰åˆ›å»ºç‹¬ç«‹çš„ç”¨ä¾‹ã€‚\nå¿…é¡»ä¸ºæ¯ä¸ªæ— æ•ˆè¾“å…¥ç±»å‹ï¼ˆå¦‚è´Ÿæ•°, å¤§äºä¸Šé™, å­—æ¯, ç‰¹æ®Šå­—ç¬¦, ç©ºå€¼ï¼‰åˆ›å»ºç‹¬ç«‹çš„ç”¨ä¾‹ã€‚\nã€å…¨é¢æ€§åŸåˆ™ã€‘ï¼šâ€œè¶…è¶Šçº¯ç²¹åŠŸèƒ½â€\n\né™¤äº†åŸºæœ¬çš„åŠŸèƒ½é€»è¾‘ï¼Œä½ å¿…é¡»ä¸»åŠ¨æ€è€ƒå¹¶ç”Ÿæˆä»¥ä¸‹ç±»å‹çš„æµ‹è¯•ç”¨ä¾‹ï¼š\nUIäº¤äº’ç”¨ä¾‹ï¼šå¦‚Hoveræç¤ºã€æŒ‰é’®çš„ç½®ç°/æ¿€æ´»çŠ¶æ€ã€é»˜è®¤å€¼æ˜¾ç¤ºç­‰ã€‚\næƒé™æ§åˆ¶ç”¨ä¾‹ï¼šä¸åŒæƒé™ç”¨æˆ·è®¿é—®åŒä¸€åŠŸèƒ½æ—¶çš„è¡¨ç°ã€‚\nå¹¶å‘æ“ä½œç”¨ä¾‹ï¼šå¤šä¸ªç”¨æˆ·åŒæ—¶æ“ä½œåŒä¸€èµ„æºå¯èƒ½äº§ç”Ÿçš„é—®é¢˜ã€‚\næ ¸å¿ƒå¼‚å¸¸åœºæ™¯ç”¨ä¾‹ï¼šå¦‚ç½‘ç»œä¸­æ–­ã€æœåŠ¡å™¨è¿”å›é”™è¯¯ç ã€ä»»åŠ¡æ‰§è¡Œå¤±è´¥/è¶…æ—¶ç­‰æƒ…å†µä¸‹çš„å‰ç«¯è¡¨ç°ã€‚\nå…¼å®¹æ€§ä¸å“åº”å¼å¸ƒå±€ç”¨ä¾‹ï¼šåœ¨ä¸åŒæµè§ˆå™¨æˆ–åˆ†è¾¨ç‡ä¸‹çš„åŸºæœ¬è¡¨ç°ã€‚\nã€è¾“å‡ºè¦æ±‚ã€‘\n\nå°†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ•´åˆåˆ°ä¸€ä¸ªMarkdownè¡¨æ ¼ä¸­ç»Ÿä¸€è¾“å‡ºã€‚\nä¸é™åˆ¶ç”¨ä¾‹æ•°é‡ï¼Œç›®æ ‡æ˜¯â€œç©·å°½â€æ‰€æœ‰å¯æµ‹ç‚¹ï¼Œä¸€æ¬¡æ€§å…¨éƒ¨åˆå¹¶ç”Ÿæˆã€‚\n\n### 3.3 æµ‹è¯•ç”¨ä¾‹è¾“å‡ºæ ¼å¼\n\næµ‹è¯•ç”¨ä¾‹è¡¨æ ¼ä¸­ä»…åŒ…å«å®Œæ•´æµ‹è¯•ç”¨ä¾‹ï¼Œæ— éœ€æ·»åŠ åˆ†ç±»ç­‰å…¶ä»–æ ‡è¯†ä¿¡æ¯\n\næµ‹è¯•ç”¨ä¾‹è¡¨æ ¼æ•°æ®ä¸­æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å¿…é¡»ä¸”ä»…åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n\n1. **ç”¨ä¾‹åç§°**ï¼šç®€æ´æ˜ç¡®åœ°æè¿°æµ‹è¯•ç›®çš„\n2. **æ‰€å±æ¨¡å—**ï¼šé‡‡ç”¨ `/æ¨¡å—/å­æ¨¡å—` çš„å‘½åè§„åˆ™ï¼ˆå¦‚ï¼š`/é—®é¢˜ç®¡ç†/é—®é¢˜åˆ—è¡¨`ï¼‰\n3. **æ ‡ç­¾**ï¼šä» `UIæµ‹è¯•`, `åŠŸèƒ½æµ‹è¯•`, `è¾¹ç•Œæµ‹è¯•`, `å¼‚å¸¸æµ‹è¯•`, `åœºæ™¯æµ‹è¯•` ä¸­é€‰æ‹©\n4. **å‰ç½®æ¡ä»¶**ï¼šæ‰§è¡Œæµ‹è¯•å‰éœ€è¦æ»¡è¶³çš„ç¯å¢ƒå’Œæ•°æ®æ¡ä»¶ï¼ˆåŒ…å«å…·ä½“çš„è¾“å…¥å€¼ï¼‰\n5. **æ­¥éª¤æè¿°**ï¼šæä¾›è¯¦ç»†ã€æ¸…æ™°ã€å¯å¤ç°çš„æ“ä½œæ­¥éª¤\n6. **é¢„æœŸç»“æœ**ï¼šæè¿°æ˜ç¡®çš„éªŒè¯ç‚¹å’ŒæœŸæœ›è¾“å‡ºï¼Œç¡®ä¿ç»“æœå¯è¢«éªŒè¯\n7. **ç¼–è¾‘æ¨¡å¼**ï¼šé»˜è®¤å¡«å……ä¸º TEXT\n8. **å¤‡æ³¨**ï¼šæä¾›ç‰¹æ®Šè¯´æ˜æˆ–æ³¨æ„äº‹é¡¹ï¼ˆå¦‚\"éœ€ä¸éœ€æ±‚æ–¹ç¡®è®¤\"ã€\"åŸºäºåˆç†å‡è®¾\"ç­‰ï¼‰\n9. **ç”¨ä¾‹ç­‰çº§**ï¼šP0 (æ ¸å¿ƒ)ã€P1 (é‡è¦)ã€P2 (æ¬¡è¦)ã€P3 (ä¼˜åŒ–)\n\n**ç‰¹æ®Šè¯´æ˜ï¼š**\n\n- å¦‚æœåœ¨éœ€æ±‚æè¿°ä¸­å­˜åœ¨è¡¨æ ¼åˆ†éš”ç¬¦å¦‚ï¼š'|'ï¼Œå°†å…¶æ›¿æ¢ä¸ºï¼š'&'ï¼Œæ–¹ä¾¿åç»­æ–‡ä»¶å¯¼å‡º\n- å¯¹äºåŸºäºå‡è®¾è®¾è®¡çš„ç”¨ä¾‹ï¼Œå¿…é¡»åœ¨å¤‡æ³¨ä¸­æ˜ç¡®æ ‡æ³¨\n- å¯¹äºéœ€è¦æ¾„æ¸…çš„åŠŸèƒ½ç‚¹ï¼Œåœ¨å¤‡æ³¨ä¸­æ³¨æ˜\"å¾…ç¡®è®¤ï¼šXXX\"\n\n---\n\n## **æ­¥éª¤4ï¼šæµ‹è¯•è¦†ç›–åº¦æ€»ç»“**\n\nåœ¨æ‰€æœ‰ç”¨ä¾‹ç”Ÿæˆå®Œæ¯•åï¼Œæä¾›ä¸€ä¸ªå®Œæ•´çš„æ€»ç»“æŠ¥å‘Šï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰ã€‚\n\n### 4.1 å…³é”®åŠŸèƒ½ç‚¹è¦†ç›–æƒ…å†µè¯´æ˜\n\nä»¥ç»“æ„åŒ–æ–¹å¼åˆ—å‡ºæ‰€æœ‰æ¨¡å—çš„è¦†ç›–æƒ…å†µï¼š\n\n#### âœ… å·²å®Œæ•´è¦†ç›–çš„æ¨¡å—\n\n- åˆ—å‡ºå®Œå…¨åŸºäºæ–‡æ¡£è®¾è®¡çš„æ¨¡å—\n- è¯´æ˜ç”¨ä¾‹æ•°é‡å’Œè¦†ç›–ç‡\n\n#### âš ï¸ éƒ¨åˆ†è¦†ç›–çš„æ¨¡å—ï¼ˆåŸºäºåˆç†å‡è®¾è®¾è®¡ï¼‰\n\n- åˆ—å‡ºéƒ¨åˆ†åŸºäºæ¨å¯¼è®¾è®¡çš„æ¨¡å—\n- è¯´æ˜å‡è®¾ä¾æ®å’Œéœ€è¦ç¡®è®¤çš„å†…å®¹\n\n#### âŒ æœªè¦†ç›–çš„æ¨¡å—ï¼ˆæ–‡æ¡£ç¼ºå¤±ï¼‰\n\n- åˆ—å‡ºå› æ–‡æ¡£ç¼ºå¤±è€Œæ— æ³•è®¾è®¡ç”¨ä¾‹çš„æ¨¡å—\n\n### 4.2 ç”¨ä¾‹ç»Ÿè®¡ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰\n\n- æ€»ç”¨ä¾‹æ•°åŠä¼˜å…ˆçº§åˆ†å¸ƒ\n- ç”¨ä¾‹ç±»å‹åˆ†å¸ƒ\n- è¦†ç›–æ¨¡å—ç»Ÿè®¡\n\n### 4.3 æ˜ç¡®åˆ—å‡ºé‡‡çº³çš„åˆç†å‡è®¾ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰\n\næŒ‰ä¼˜å…ˆçº§åˆ—å‡ºæ‰€æœ‰å‡è®¾ï¼š\n\n- å‡è®¾å†…å®¹\n- å‡è®¾ä¾æ®\n- å½±å“èŒƒå›´\n- é£é™©è¯„ä¼°\n\n### 4.4 è´¨é‡æ ‡å‡†ä½“ç°è¯´æ˜ï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰\n\nè¯´æ˜æœ¬æ¬¡è®¾è®¡å¦‚ä½•ä½“ç°ï¼š\n\n- å®Œæ•´æ€§\n- å¯æ‰§è¡Œæ€§\n- ç‹¬ç«‹æ€§\n- å¯ç»´æŠ¤æ€§\n- ä¼˜å…ˆçº§åˆç†æ€§\n\n### 4.5 æ½œåœ¨é£é™©åŒºåŸŸï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰\n\nåˆ—å‡ºï¼š\n\n- å› éœ€æ±‚ä¸æ˜ç¡®è€Œæœªå®Œå…¨è¦†ç›–çš„åŒºåŸŸ\n- å› å¤æ‚åº¦è¿‡é«˜è€Œéœ€è¦ç‰¹åˆ«å…³æ³¨çš„åŒºåŸŸ\n- æµ‹è¯•å»ºè®®\n\n---\n\n## è¾“å…¥ä¿¡æ¯æŒ‡å—\n\nä¸ºç¡®ä¿ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œ**å»ºè®®ç”¨æˆ·åœ¨æä¾›çš„éœ€æ±‚æ–‡æ¡£ä¸­å°½å¯èƒ½åŒ…å«ä»¥ä¸‹ä¿¡æ¯**ï¼š\n\n1. **åŠŸèƒ½æ¨¡å—åç§°**ï¼šè¢«æµ‹åŠŸèƒ½çš„å…·ä½“åç§°\n2. **è¯¦ç»†åŠŸèƒ½æè¿°**ï¼šæ¸…æ™°æè¿°åŠŸèƒ½çš„ç›®æ ‡ã€ç”¨æˆ·åœºæ™¯å’Œä¸šåŠ¡æµç¨‹\n3. **é¡µé¢ç»“æ„è¯´æ˜**ï¼šæ˜ç¡®è¯´æ˜æœ‰å“ªäº›é¡µé¢ï¼ˆåˆ—è¡¨é¡µã€è¯¦æƒ…é¡µç­‰ï¼‰åŠå…¶äº¤äº’å…³ç³»\n4. **ä¸šåŠ¡è§„åˆ™**ï¼šå…·ä½“çš„ä¸šåŠ¡é€»è¾‘å’Œé™åˆ¶æ¡ä»¶\n5. **æ¥å£/UIå…ƒç´ **ï¼šè¾“å…¥å‚æ•°çš„ç±»å‹ã€æ ¼å¼ã€å–å€¼èŒƒå›´å’Œçº¦æŸæ¡ä»¶\n6. **é¢„æœŸç»“æœ**ï¼šæˆåŠŸçš„è¾“å‡ºç»“æœå’Œç³»ç»Ÿè¡Œä¸ºè¡¨ç°\n7. **å‰ç½®æ¡ä»¶ä¸ä¾èµ–**ï¼šæ‰§è¡Œè¯¥åŠŸèƒ½å‰éœ€è¦æ»¡è¶³çš„æ¡ä»¶æˆ–ä¾èµ–çš„å…¶ä»–æ¨¡å—\n8. **å¼‚å¸¸å¤„ç†æœºåˆ¶**ï¼šå·²çŸ¥çš„é”™è¯¯æƒ…å†µå’Œå¯¹åº”çš„å¤„ç†æ–¹å¼\n\n---\n\n## è´¨é‡æ ‡å‡†\n\nç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹å¿…é¡»æ»¡è¶³ä»¥ä¸‹æ ¸å¿ƒæ ‡å‡†ï¼š\n\n- **å¯æ‰§è¡Œæ€§**ï¼šæ­¥éª¤æ¸…æ™°ï¼Œä»»ä½•æµ‹è¯•äººå‘˜éƒ½èƒ½æŒ‰æ­¥éª¤æ‰§è¡Œ\n- **ç‹¬ç«‹æ€§**ï¼šæ¯ä¸ªç”¨ä¾‹éƒ½èƒ½ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–å…¶ä»–ç”¨ä¾‹çš„æ‰§è¡Œç»“æœ\n- **å®Œæ•´æ€§**ï¼šè¦†ç›–æ­£å¸¸æµç¨‹ã€å¼‚å¸¸æµç¨‹ã€è¾¹ç•Œæ¡ä»¶å’Œé”™è¯¯å¤„ç†\n- **å¯éªŒè¯æ€§**ï¼šé¢„æœŸç»“æœæ˜ç¡®å…·ä½“ï¼Œä¾¿äºåˆ¤æ–­æµ‹è¯•é€šè¿‡æˆ–å¤±è´¥\n- **å¯ç»´æŠ¤æ€§**ï¼šç”¨ä¾‹ç»“æ„æ¸…æ™°ï¼Œä¾¿äºåç»­ä¿®æ”¹å’Œç»´æŠ¤\n- **å¯è¿½æº¯æ€§**ï¼šç”¨ä¾‹ä¸éœ€æ±‚ç‚¹æ˜ç¡®å¯¹åº”ï¼Œä¾¿äºè¿½è¸ªè¦†ç›–æƒ…å†µ"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "selected_output": "prompt",
          "type": "Prompt"
        },
        "dragging": false,
        "height": 260,
        "id": "Prompt-N7xxx",
        "measured": {
          "height": 260,
          "width": 320
        },
        "position": {
          "x": 294.60700411037885,
          "y": 982.8714162625467
        },
        "positionAbsolute": {
          "x": 690.2015147036818,
          "y": 1018.5443911764344
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "File-YG1BS",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Loads content from one or more files.",
            "display_name": "File",
            "documentation": "https://docs.langflow.org/components-data#file",
            "edited": false,
            "field_order": [
              "path",
              "file_path",
              "separator",
              "silent_errors",
              "delete_server_file_after_processing",
              "ignore_unsupported_extensions",
              "ignore_unspecified_files",
              "advanced_mode",
              "pipeline",
              "ocr_engine",
              "md_image_placeholder",
              "md_page_break_placeholder",
              "doc_key",
              "use_multithreading",
              "concurrency_multithreading",
              "markdown"
            ],
            "frozen": false,
            "icon": "file-text",
            "last_updated": "2025-11-26T08:38:54.467Z",
            "legacy": false,
            "lf_version": "1.6.4",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Raw Content",
                "group_outputs": false,
                "hidden": null,
                "method": "load_files_message",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "advanced_mode": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Advanced Parser",
                "dynamic": false,
                "info": "Enable advanced document processing and export with Docling for PDFs, images, and office documents. Available only for single file processing.Note that advanced document processing can consume significant resources.",
                "list": false,
                "list_add_label": "Add More",
                "name": "advanced_mode",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "\"\"\"Enhanced file component with Docling support and process isolation.\n\nNotes:\n-----\n- ALL Docling parsing/export runs in a separate OS process to prevent memory\n  growth and native library state from impacting the main Langflow process.\n- Standard text/structured parsing continues to use existing BaseFileComponent\n  utilities (and optional threading via `parallel_load_data`).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport subprocess\nimport sys\nimport textwrap\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\n\nfrom langflow.base.data.base_file import BaseFileComponent\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.io import (\n    BoolInput,\n    DropdownInput,\n    FileInput,\n    IntInput,\n    MessageTextInput,\n    Output,\n    StrInput,\n)\nfrom langflow.schema.data import Data\nfrom langflow.schema.message import Message\n\nif TYPE_CHECKING:\n    from langflow.schema import DataFrame\n\n\nclass FileComponent(BaseFileComponent):\n    \"\"\"File component with optional Docling processing (isolated in a subprocess).\"\"\"\n\n    display_name = \"File\"\n    description = \"Loads content from one or more files.\"\n    documentation: str = \"https://docs.langflow.org/components-data#file\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    # Docling-supported/compatible extensions; TEXT_FILE_TYPES are supported by the base loader.\n    VALID_EXTENSIONS = [\n        *TEXT_FILE_TYPES,\n        \"adoc\",\n        \"asciidoc\",\n        \"asc\",\n        \"bmp\",\n        \"dotx\",\n        \"dotm\",\n        \"docm\",\n        \"jpeg\",\n        \"png\",\n        \"potx\",\n        \"ppsx\",\n        \"pptm\",\n        \"potm\",\n        \"ppsm\",\n        \"pptx\",\n        \"tiff\",\n        \"xls\",\n        \"xlsx\",\n        \"xhtml\",\n        \"webp\",\n    ]\n\n    # Fixed export settings used when markdown export is requested.\n    EXPORT_FORMAT = \"Markdown\"\n    IMAGE_MODE = \"placeholder\"\n\n    # ---- Inputs / Outputs (kept as close to original as possible) -------------------\n    _base_inputs = deepcopy(BaseFileComponent._base_inputs)\n    for input_item in _base_inputs:\n        if isinstance(input_item, FileInput) and input_item.name == \"path\":\n            input_item.real_time_refresh = True\n            break\n\n    inputs = [\n        *_base_inputs,\n        BoolInput(\n            name=\"advanced_mode\",\n            display_name=\"Advanced Parser\",\n            value=False,\n            real_time_refresh=True,\n            info=(\n                \"Enable advanced document processing and export with Docling for PDFs, images, and office documents. \"\n                \"Available only for single file processing.\"\n                \"Note that advanced document processing can consume significant resources.\"\n            ),\n            show=False,\n        ),\n        DropdownInput(\n            name=\"pipeline\",\n            display_name=\"Pipeline\",\n            info=\"Docling pipeline to use\",\n            options=[\"standard\", \"vlm\"],\n            value=\"standard\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"ocr_engine\",\n            display_name=\"OCR Engine\",\n            info=\"OCR engine to use. Only available when pipeline is set to 'standard'.\",\n            options=[\"None\", \"easyocr\"],\n            value=\"easyocr\",\n            show=False,\n            advanced=True,\n        ),\n        StrInput(\n            name=\"md_image_placeholder\",\n            display_name=\"Image placeholder\",\n            info=\"Specify the image placeholder for markdown exports.\",\n            value=\"<!-- image -->\",\n            advanced=True,\n            show=False,\n        ),\n        StrInput(\n            name=\"md_page_break_placeholder\",\n            display_name=\"Page break placeholder\",\n            info=\"Add this placeholder between pages in the markdown output.\",\n            value=\"\",\n            advanced=True,\n            show=False,\n        ),\n        MessageTextInput(\n            name=\"doc_key\",\n            display_name=\"Doc Key\",\n            info=\"The key to use for the DoclingDocument column.\",\n            value=\"doc\",\n            advanced=True,\n            show=False,\n        ),\n        # Deprecated input retained for backward-compatibility.\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"[Deprecated] Use Multithreading\",\n            advanced=True,\n            value=True,\n            info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Processing Concurrency\",\n            advanced=True,\n            info=\"When multiple files are being processed, the number of files to process concurrently.\",\n            value=1,\n        ),\n        BoolInput(\n            name=\"markdown\",\n            display_name=\"Markdown Export\",\n            info=\"Export processed documents to Markdown format. Only available when advanced mode is enabled.\",\n            value=False,\n            show=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Raw Content\", name=\"message\", method=\"load_files_message\"),\n    ]\n\n    # ------------------------------ UI helpers --------------------------------------\n\n    def _path_value(self, template: dict) -> list[str]:\n        \"\"\"Return the list of currently selected file paths from the template.\"\"\"\n        return template.get(\"path\", {}).get(\"file_path\", [])\n\n    def update_build_config(\n        self,\n        build_config: dict[str, Any],\n        field_value: Any,\n        field_name: str | None = None,\n    ) -> dict[str, Any]:\n        \"\"\"Show/hide Advanced Parser and related fields based on selection context.\"\"\"\n        if field_name == \"path\":\n            paths = self._path_value(build_config)\n            file_path = paths[0] if paths else \"\"\n            file_count = len(field_value) if field_value else 0\n\n            # Advanced mode only for single (non-tabular) file\n            allow_advanced = file_count == 1 and not file_path.endswith((\".csv\", \".xlsx\", \".parquet\"))\n            build_config[\"advanced_mode\"][\"show\"] = allow_advanced\n            if not allow_advanced:\n                build_config[\"advanced_mode\"][\"value\"] = False\n                for f in (\"pipeline\", \"ocr_engine\", \"doc_key\", \"md_image_placeholder\", \"md_page_break_placeholder\"):\n                    if f in build_config:\n                        build_config[f][\"show\"] = False\n\n        # Docling Processing\n        elif field_name == \"advanced_mode\":\n            for f in (\"pipeline\", \"ocr_engine\", \"doc_key\", \"md_image_placeholder\", \"md_page_break_placeholder\"):\n                if f in build_config:\n                    build_config[f][\"show\"] = bool(field_value)\n\n        elif field_name == \"pipeline\":\n            if field_value == \"standard\":\n                build_config[\"ocr_engine\"][\"show\"] = True\n                build_config[\"ocr_engine\"][\"value\"] = \"easyocr\"\n            else:\n                build_config[\"ocr_engine\"][\"show\"] = False\n                build_config[\"ocr_engine\"][\"value\"] = \"None\"\n\n        return build_config\n\n    def update_outputs(self, frontend_node: dict[str, Any], field_name: str, field_value: Any) -> dict[str, Any]:  # noqa: ARG002\n        \"\"\"Dynamically show outputs based on file count/type and advanced mode.\"\"\"\n        if field_name not in [\"path\", \"advanced_mode\", \"pipeline\"]:\n            return frontend_node\n\n        template = frontend_node.get(\"template\", {})\n        paths = self._path_value(template)\n        if not paths:\n            return frontend_node\n\n        frontend_node[\"outputs\"] = []\n        if len(paths) == 1:\n            file_path = paths[0] if field_name == \"path\" else frontend_node[\"template\"][\"path\"][\"file_path\"][0]\n            if file_path.endswith((\".csv\", \".xlsx\", \".parquet\")):\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Structured Content\", name=\"dataframe\", method=\"load_files_structured\"),\n                )\n            elif file_path.endswith(\".json\"):\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Structured Content\", name=\"json\", method=\"load_files_json\"),\n                )\n\n            advanced_mode = frontend_node.get(\"template\", {}).get(\"advanced_mode\", {}).get(\"value\", False)\n            if advanced_mode:\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Structured Output\", name=\"advanced_dataframe\", method=\"load_files_dataframe\"),\n                )\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Markdown\", name=\"advanced_markdown\", method=\"load_files_markdown\"),\n                )\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"File Path\", name=\"path\", method=\"load_files_path\"),\n                )\n            else:\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"Raw Content\", name=\"message\", method=\"load_files_message\"),\n                )\n                frontend_node[\"outputs\"].append(\n                    Output(display_name=\"File Path\", name=\"path\", method=\"load_files_path\"),\n                )\n        else:\n            # Multiple files => DataFrame output; advanced parser disabled\n            frontend_node[\"outputs\"].append(Output(display_name=\"Files\", name=\"dataframe\", method=\"load_files\"))\n\n        return frontend_node\n\n    # ------------------------------ Core processing ----------------------------------\n\n    def _is_docling_compatible(self, file_path: str) -> bool:\n        \"\"\"Lightweight extension gate for Docling-compatible types.\"\"\"\n        docling_exts = (\n            \".adoc\",\n            \".asciidoc\",\n            \".asc\",\n            \".bmp\",\n            \".csv\",\n            \".dotx\",\n            \".dotm\",\n            \".docm\",\n            \".docx\",\n            \".htm\",\n            \".html\",\n            \".jpeg\",\n            \".json\",\n            \".md\",\n            \".pdf\",\n            \".png\",\n            \".potx\",\n            \".ppsx\",\n            \".pptm\",\n            \".potm\",\n            \".ppsm\",\n            \".pptx\",\n            \".tiff\",\n            \".txt\",\n            \".xls\",\n            \".xlsx\",\n            \".xhtml\",\n            \".xml\",\n            \".webp\",\n        )\n        return file_path.lower().endswith(docling_exts)\n\n    def _process_docling_in_subprocess(self, file_path: str) -> Data | None:\n        \"\"\"Run Docling in a separate OS process and map the result to a Data object.\n\n        We avoid multiprocessing pickling by launching `python -c \"<script>\"` and\n        passing JSON config via stdin. The child prints a JSON result to stdout.\n        \"\"\"\n        if not file_path:\n            return None\n\n        args: dict[str, Any] = {\n            \"file_path\": file_path,\n            \"markdown\": bool(self.markdown),\n            \"image_mode\": str(self.IMAGE_MODE),\n            \"md_image_placeholder\": str(self.md_image_placeholder),\n            \"md_page_break_placeholder\": str(self.md_page_break_placeholder),\n            \"pipeline\": str(self.pipeline),\n            \"ocr_engine\": (\n                self.ocr_engine if self.ocr_engine and self.ocr_engine != \"None\" and self.pipeline != \"vlm\" else None\n            ),\n        }\n\n        self.log(f\"Starting Docling subprocess for file: {file_path}\")\n        self.log(args)\n\n        # Child script for isolating the docling processing\n        child_script = textwrap.dedent(\n            r\"\"\"\n            import json, sys\n\n            def try_imports():\n                # Strategy 1: latest layout\n                try:\n                    from docling.datamodel.base_models import ConversionStatus, InputFormat  # type: ignore\n                    from docling.document_converter import DocumentConverter  # type: ignore\n                    from docling_core.types.doc import ImageRefMode  # type: ignore\n                    return ConversionStatus, InputFormat, DocumentConverter, ImageRefMode, \"latest\"\n                except Exception:\n                    pass\n                # Strategy 2: alternative layout\n                try:\n                    from docling.document_converter import DocumentConverter  # type: ignore\n                    try:\n                        from docling_core.types import ConversionStatus, InputFormat  # type: ignore\n                    except Exception:\n                        try:\n                            from docling.datamodel import ConversionStatus, InputFormat  # type: ignore\n                        except Exception:\n                            class ConversionStatus: SUCCESS = \"success\"\n                            class InputFormat:\n                                PDF=\"pdf\"; IMAGE=\"image\"\n                    try:\n                        from docling_core.types.doc import ImageRefMode  # type: ignore\n                    except Exception:\n                        class ImageRefMode:\n                            PLACEHOLDER=\"placeholder\"; EMBEDDED=\"embedded\"\n                    return ConversionStatus, InputFormat, DocumentConverter, ImageRefMode, \"alternative\"\n                except Exception:\n                    pass\n                # Strategy 3: basic converter only\n                try:\n                    from docling.document_converter import DocumentConverter  # type: ignore\n                    class ConversionStatus: SUCCESS = \"success\"\n                    class InputFormat:\n                        PDF=\"pdf\"; IMAGE=\"image\"\n                    class ImageRefMode:\n                        PLACEHOLDER=\"placeholder\"; EMBEDDED=\"embedded\"\n                    return ConversionStatus, InputFormat, DocumentConverter, ImageRefMode, \"basic\"\n                except Exception as e:\n                    raise ImportError(f\"Docling imports failed: {e}\") from e\n\n            def create_converter(strategy, input_format, DocumentConverter, pipeline, ocr_engine):\n                # --- Standard PDF/IMAGE pipeline (your existing behavior), with optional OCR ---\n                if pipeline == \"standard\":\n                    try:\n                        from docling.datamodel.pipeline_options import PdfPipelineOptions  # type: ignore\n                        from docling.document_converter import PdfFormatOption  # type: ignore\n\n                        pipe = PdfPipelineOptions()\n                        pipe.do_ocr = False\n\n                        if ocr_engine:\n                            try:\n                                from docling.models.factories import get_ocr_factory  # type: ignore\n                                pipe.do_ocr = True\n                                fac = get_ocr_factory(allow_external_plugins=False)\n                                pipe.ocr_options = fac.create_options(kind=ocr_engine)\n                            except Exception:\n                                # If OCR setup fails, disable it\n                                pipe.do_ocr = False\n\n                        fmt = {}\n                        if hasattr(input_format, \"PDF\"):\n                            fmt[getattr(input_format, \"PDF\")] = PdfFormatOption(pipeline_options=pipe)\n                        if hasattr(input_format, \"IMAGE\"):\n                            fmt[getattr(input_format, \"IMAGE\")] = PdfFormatOption(pipeline_options=pipe)\n\n                        return DocumentConverter(format_options=fmt)\n                    except Exception:\n                        return DocumentConverter()\n\n                # --- Vision-Language Model (VLM) pipeline ---\n                if pipeline == \"vlm\":\n                    try:\n                        from docling.pipeline.vlm_pipeline import VlmPipeline\n                        from docling.document_converter import PdfFormatOption  # type: ignore\n\n                        vl_pipe = VlmPipelineOptions()\n\n                        # VLM paths generally don't need OCR; keep OCR off by default here.\n                        fmt = {}\n                        if hasattr(input_format, \"PDF\"):\n                            fmt[getattr(input_format, \"PDF\")] = PdfFormatOption(pipeline_cls=VlmPipeline)\n                        if hasattr(input_format, \"IMAGE\"):\n                            fmt[getattr(input_format, \"IMAGE\")] = PdfFormatOption(pipeline_cls=VlmPipeline)\n\n                        return DocumentConverter(format_options=fmt)\n                    except Exception:\n                        return DocumentConverter()\n\n                # --- Fallback: default converter with no special options ---\n                return DocumentConverter()\n\n            def export_markdown(document, ImageRefMode, image_mode, img_ph, pg_ph):\n                try:\n                    mode = getattr(ImageRefMode, image_mode.upper(), image_mode)\n                    return document.export_to_markdown(\n                        image_mode=mode,\n                        image_placeholder=img_ph,\n                        page_break_placeholder=pg_ph,\n                    )\n                except Exception:\n                    try:\n                        return document.export_to_text()\n                    except Exception:\n                        return str(document)\n\n            def to_rows(doc_dict):\n                rows = []\n                for t in doc_dict.get(\"texts\", []):\n                    prov = t.get(\"prov\") or []\n                    page_no = None\n                    if prov and isinstance(prov, list) and isinstance(prov[0], dict):\n                        page_no = prov[0].get(\"page_no\")\n                    rows.append({\n                        \"page_no\": page_no,\n                        \"label\": t.get(\"label\"),\n                        \"text\": t.get(\"text\"),\n                        \"level\": t.get(\"level\"),\n                    })\n                return rows\n\n            def main():\n                cfg = json.loads(sys.stdin.read())\n                file_path = cfg[\"file_path\"]\n                markdown = cfg[\"markdown\"]\n                image_mode = cfg[\"image_mode\"]\n                img_ph = cfg[\"md_image_placeholder\"]\n                pg_ph = cfg[\"md_page_break_placeholder\"]\n                pipeline = cfg[\"pipeline\"]\n                ocr_engine = cfg.get(\"ocr_engine\")\n                meta = {\"file_path\": file_path}\n\n                try:\n                    ConversionStatus, InputFormat, DocumentConverter, ImageRefMode, strategy = try_imports()\n                    converter = create_converter(strategy, InputFormat, DocumentConverter, pipeline, ocr_engine)\n                    try:\n                        res = converter.convert(file_path)\n                    except Exception as e:\n                        print(json.dumps({\"ok\": False, \"error\": f\"Docling conversion error: {e}\", \"meta\": meta}))\n                        return\n\n                    ok = False\n                    if hasattr(res, \"status\"):\n                        try:\n                            ok = (res.status == ConversionStatus.SUCCESS) or (str(res.status).lower() == \"success\")\n                        except Exception:\n                            ok = (str(res.status).lower() == \"success\")\n                    if not ok and hasattr(res, \"document\"):\n                        ok = getattr(res, \"document\", None) is not None\n                    if not ok:\n                        print(json.dumps({\"ok\": False, \"error\": \"Docling conversion failed\", \"meta\": meta}))\n                        return\n\n                    doc = getattr(res, \"document\", None)\n                    if doc is None:\n                        print(json.dumps({\"ok\": False, \"error\": \"Docling produced no document\", \"meta\": meta}))\n                        return\n\n                    if markdown:\n                        text = export_markdown(doc, ImageRefMode, image_mode, img_ph, pg_ph)\n                        print(json.dumps({\"ok\": True, \"mode\": \"markdown\", \"text\": text, \"meta\": meta}))\n                        return\n\n                    # structured\n                    try:\n                        doc_dict = doc.export_to_dict()\n                    except Exception as e:\n                        print(json.dumps({\"ok\": False, \"error\": f\"Docling export_to_dict failed: {e}\", \"meta\": meta}))\n                        return\n\n                    rows = to_rows(doc_dict)\n                    print(json.dumps({\"ok\": True, \"mode\": \"structured\", \"doc\": rows, \"meta\": meta}))\n                except Exception as e:\n                    print(\n                        json.dumps({\n                            \"ok\": False,\n                            \"error\": f\"Docling processing error: {e}\",\n                            \"meta\": {\"file_path\": file_path},\n                        })\n                    )\n\n            if __name__ == \"__main__\":\n                main()\n            \"\"\"\n        )\n\n        # Validate file_path to avoid command injection or unsafe input\n        if not isinstance(args[\"file_path\"], str) or any(c in args[\"file_path\"] for c in [\";\", \"|\", \"&\", \"$\", \"`\"]):\n            return Data(data={\"error\": \"Unsafe file path detected.\", \"file_path\": args[\"file_path\"]})\n\n        proc = subprocess.run(  # noqa: S603\n            [sys.executable, \"-u\", \"-c\", child_script],\n            input=json.dumps(args).encode(\"utf-8\"),\n            capture_output=True,\n            check=False,\n        )\n\n        if not proc.stdout:\n            err_msg = proc.stderr.decode(\"utf-8\", errors=\"replace\") or \"no output from child process\"\n            return Data(data={\"error\": f\"Docling subprocess error: {err_msg}\", \"file_path\": file_path})\n\n        try:\n            result = json.loads(proc.stdout.decode(\"utf-8\"))\n        except Exception as e:  # noqa: BLE001\n            err_msg = proc.stderr.decode(\"utf-8\", errors=\"replace\")\n            return Data(\n                data={\"error\": f\"Invalid JSON from Docling subprocess: {e}. stderr={err_msg}\", \"file_path\": file_path},\n            )\n\n        if not result.get(\"ok\"):\n            return Data(data={\"error\": result.get(\"error\", \"Unknown Docling error\"), **result.get(\"meta\", {})})\n\n        meta = result.get(\"meta\", {})\n        if result.get(\"mode\") == \"markdown\":\n            exported_content = str(result.get(\"text\", \"\"))\n            return Data(\n                text=exported_content,\n                data={\"exported_content\": exported_content, \"export_format\": self.EXPORT_FORMAT, **meta},\n            )\n\n        rows = list(result.get(\"doc\", []))\n        return Data(data={\"doc\": rows, \"export_format\": self.EXPORT_FORMAT, **meta})\n\n    def process_files(\n        self,\n        file_list: list[BaseFileComponent.BaseFile],\n    ) -> list[BaseFileComponent.BaseFile]:\n        \"\"\"Process input files.\n\n        - Single file + advanced_mode => Docling in a separate process.\n        - Otherwise => standard parsing in current process (optionally threaded).\n        \"\"\"\n        if not file_list:\n            msg = \"No files to process.\"\n            raise ValueError(msg)\n\n        def process_file_standard(file_path: str, *, silent_errors: bool = False) -> Data | None:\n            try:\n                return parse_text_file_to_data(file_path, silent_errors=silent_errors)\n            except FileNotFoundError as e:\n                self.log(f\"File not found: {file_path}. Error: {e}\")\n                if not silent_errors:\n                    raise\n                return None\n            except Exception as e:\n                self.log(f\"Unexpected error processing {file_path}: {e}\")\n                if not silent_errors:\n                    raise\n                return None\n\n        # Advanced path: only for a single Docling-compatible file\n        if len(file_list) == 1:\n            file_path = str(file_list[0].path)\n            if self.advanced_mode and self._is_docling_compatible(file_path):\n                advanced_data: Data | None = self._process_docling_in_subprocess(file_path)\n\n                # --- UNNEST: expand each element in `doc` to its own Data row\n                payload = getattr(advanced_data, \"data\", {}) or {}\n                doc_rows = payload.get(\"doc\")\n                if isinstance(doc_rows, list):\n                    rows: list[Data | None] = [\n                        Data(\n                            data={\n                                \"file_path\": file_path,\n                                **(item if isinstance(item, dict) else {\"value\": item}),\n                            },\n                        )\n                        for item in doc_rows\n                    ]\n                    return self.rollup_data(file_list, rows)\n\n                # If not structured, keep as-is (e.g., markdown export or error dict)\n                return self.rollup_data(file_list, [advanced_data])\n\n        # Standard multi-file (or single non-advanced) path\n        concurrency = 1 if not self.use_multithreading else max(1, self.concurrency_multithreading)\n        file_paths = [str(f.path) for f in file_list]\n        self.log(f\"Starting parallel processing of {len(file_paths)} files with concurrency: {concurrency}.\")\n        my_data = parallel_load_data(\n            file_paths,\n            silent_errors=self.silent_errors,\n            load_function=process_file_standard,\n            max_concurrency=concurrency,\n        )\n        return self.rollup_data(file_list, my_data)\n\n    # ------------------------------ Output helpers -----------------------------------\n\n    def load_files_helper(self) -> DataFrame:\n        result = self.load_files()\n\n        # Error condition - raise error if no text and an error is present\n        if not hasattr(result, \"text\"):\n            if hasattr(result, \"error\"):\n                raise ValueError(result.error[0])\n            msg = \"No content generated.\"\n            raise ValueError(msg)\n\n        return result\n\n    def load_files_dataframe(self) -> DataFrame:\n        \"\"\"Load files using advanced Docling processing and export to DataFrame format.\"\"\"\n        self.markdown = False\n        return self.load_files_helper()\n\n    def load_files_markdown(self) -> Message:\n        \"\"\"Load files using advanced Docling processing and export to Markdown format.\"\"\"\n        self.markdown = True\n        result = self.load_files_helper()\n        return Message(text=str(result.text[0]))\n"
              },
              "concurrency_multithreading": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Processing Concurrency",
                "dynamic": false,
                "info": "When multiple files are being processed, the number of files to process concurrently.",
                "list": false,
                "list_add_label": "Add More",
                "name": "concurrency_multithreading",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "delete_server_file_after_processing": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Delete Server File After Processing",
                "dynamic": false,
                "info": "If true, the Server File Path will be deleted after processing.",
                "list": false,
                "list_add_label": "Add More",
                "name": "delete_server_file_after_processing",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "doc_key": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Doc Key",
                "dynamic": false,
                "info": "The key to use for the DoclingDocument column.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "doc_key",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "doc"
              },
              "file_path": {
                "_input_type": "HandleInput",
                "advanced": true,
                "display_name": "Server File Path",
                "dynamic": false,
                "info": "Data object with a 'file_path' property pointing to server file or a Message object with a path to the file. Supercedes 'Path' but supports same file types.",
                "input_types": [
                  "Data",
                  "Message"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "file_path",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "ignore_unspecified_files": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Ignore Unspecified Files",
                "dynamic": false,
                "info": "If true, Data with no 'file_path' property will be ignored.",
                "list": false,
                "list_add_label": "Add More",
                "name": "ignore_unspecified_files",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "ignore_unsupported_extensions": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Ignore Unsupported Extensions",
                "dynamic": false,
                "info": "If true, files with unsupported extensions will not be processed.",
                "list": false,
                "list_add_label": "Add More",
                "name": "ignore_unsupported_extensions",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "markdown": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Markdown Export",
                "dynamic": false,
                "info": "Export processed documents to Markdown format. Only available when advanced mode is enabled.",
                "list": false,
                "list_add_label": "Add More",
                "name": "markdown",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "md_image_placeholder": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Image placeholder",
                "dynamic": false,
                "info": "Specify the image placeholder for markdown exports.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "md_image_placeholder",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "<!-- image -->"
              },
              "md_page_break_placeholder": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Page break placeholder",
                "dynamic": false,
                "info": "Add this placeholder between pages in the markdown output.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "md_page_break_placeholder",
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "ocr_engine": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "OCR Engine",
                "dynamic": false,
                "external_options": {},
                "info": "OCR engine to use. Only available when pipeline is set to 'standard'.",
                "name": "ocr_engine",
                "options": [
                  "None",
                  "easyocr"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "easyocr"
              },
              "path": {
                "_input_type": "FileInput",
                "advanced": false,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "adoc",
                  "asciidoc",
                  "asc",
                  "bmp",
                  "dotx",
                  "dotm",
                  "docm",
                  "jpeg",
                  "png",
                  "potx",
                  "ppsx",
                  "pptm",
                  "potm",
                  "ppsm",
                  "pptx",
                  "tiff",
                  "xls",
                  "xlsx",
                  "xhtml",
                  "webp",
                  "zip",
                  "tar",
                  "tgz",
                  "bz2",
                  "gz"
                ],
                "file_path": [],
                "info": "Supported file extensions: csv, json, pdf, txt, md, mdx, yaml, yml, xml, html, htm, docx, py, sh, sql, js, ts, tsx, adoc, asciidoc, asc, bmp, dotx, dotm, docm, jpeg, png, potx, ppsx, pptm, potm, ppsm, pptx, tiff, xls, xlsx, xhtml, webp; optionally bundled in file extensions: zip, tar, tgz, bz2, gz",
                "list": true,
                "list_add_label": "Add More",
                "name": "path",
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "temp_file": false,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "pipeline": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Pipeline",
                "dynamic": false,
                "external_options": {},
                "info": "Docling pipeline to use",
                "name": "pipeline",
                "options": [
                  "standard",
                  "vlm"
                ],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "standard"
              },
              "separator": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "Specify the separator to use between multiple outputs in Message format.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "separator",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n\n"
              },
              "silent_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Silent Errors",
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "list": false,
                "list_add_label": "Add More",
                "name": "silent_errors",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "use_multithreading": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "[Deprecated] Use Multithreading",
                "dynamic": false,
                "info": "Set 'Processing Concurrency' greater than 1 to enable multithreading.",
                "list": false,
                "list_add_label": "Add More",
                "name": "use_multithreading",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "selected_output": "message",
          "showNode": true,
          "type": "File"
        },
        "dragging": false,
        "id": "File-YG1BS",
        "measured": {
          "height": 213,
          "width": 320
        },
        "position": {
          "x": 295.3317868137668,
          "y": 614.9213137041477
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TableToExcel-RWezt",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extract markdown tables and convert to Excel/CSV format",
            "display_name": "Table to Excel/CSV",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text"
            ],
            "frozen": false,
            "icon": "Table",
            "legacy": false,
            "lf_version": "1.6.4",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Table Data",
                "group_outputs": false,
                "hidden": null,
                "method": "extract_to_data",
                "name": "table_data",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport pandas as pd\nimport re\nfrom typing import List\n\nclass TableToCSVComponent(Component):\n    display_name = \"Table to Excel/CSV\"\n    description = \"Extract markdown tables and convert to Excel/CSV format\"\n    icon = \"Table\"\n    name = \"TableToExcel\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing markdown tables (from AI output)\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Table Data\", name=\"table_data\", method=\"extract_to_data\"),\n    ]\n\n    def clean_cell_content(self, content: str) -> str:\n        \"\"\"æ¸…ç† HTML æ ‡ç­¾å’Œ Markdown æ ¼å¼\"\"\"\n        if not content:\n            return \"\"\n\n        content = re.sub(r\"<br\\s*/?>\", \"\\n\", content)\n        content = re.sub(r\"<[^>]+>\", \"\", content)\n        content = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", content)\n        content = re.sub(r\"__(.*?)__\", r\"\\1\", content)\n        content = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", content)\n        content = re.sub(r\"_(.*?)_\", r\"\\1\", content)\n        content = re.sub(r\"~~(.*?)~~\", r\"\\1\", content)\n        content = re.sub(r\"`(.*?)`\", r\"\\1\", content)\n\n        lines = content.split(\"\\n\")\n        cleaned_lines = [\" \".join(line.split()) for line in lines]\n        content = \"\\n\".join(cleaned_lines)\n\n        return content.strip()\n\n    def is_separator_row(self, cells: List[str]) -> bool:\n        \"\"\"æ£€æŸ¥æ˜¯å¦ä¸ºè¡¨æ ¼åˆ†éš”è¡Œ\"\"\"\n        for cell in cells:\n            clean_cell = cell.replace(\":\", \"\")\n            if not re.fullmatch(r\"-+\", clean_cell.strip()):\n                return False\n        return True\n\n    def parse_table_row(self, row: str, expected_columns: int = None) -> List[str]:\n        \"\"\"æ™ºèƒ½è§£æè¡¨æ ¼è¡Œï¼Œæ­£ç¡®å¤„ç†å•å…ƒæ ¼å†…çš„|å­—ç¬¦\"\"\"\n        row = row.strip()\n\n        # ç§»é™¤é¦–å°¾çš„|\n        if row.startswith(\"|\"):\n            row = row[1:]\n        if row.endswith(\"|\"):\n            row = row[:-1]\n\n        # å¦‚æœæ²¡æœ‰æœŸæœ›çš„åˆ—æ•°ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²\n        if expected_columns is None:\n            cells = [cell.strip() for cell in row.split(\"|\")]\n            return [cell for cell in cells if cell != \"\"]\n\n        # æ™ºèƒ½åˆ†å‰²ï¼šç¡®ä¿å¾—åˆ°æ­£ç¡®çš„åˆ—æ•°\n        cells = []\n        current_cell = \"\"\n\n        i = 0\n        while i < len(row):\n            char = row[i]\n            if char == \"|\":\n                # å¦‚æœå·²ç»è¾¾åˆ°æœŸæœ›åˆ—æ•°-1ï¼Œå‰©ä½™å†…å®¹éƒ½å±äºæœ€åä¸€åˆ—\n                if len(cells) == expected_columns - 1:\n                    current_cell += row[i:]\n                    break\n                else:\n                    # å®Œæˆå½“å‰å•å…ƒæ ¼\n                    cells.append(current_cell.strip())\n                    current_cell = \"\"\n            else:\n                current_cell += char\n            i += 1\n\n        # æ·»åŠ æœ€åä¸€ä¸ªå•å…ƒæ ¼\n        if current_cell or len(cells) < expected_columns:\n            cells.append(current_cell.strip())\n\n        # ç¡®ä¿åˆ—æ•°åŒ¹é…\n        while len(cells) < expected_columns:\n            cells.append(\"\")\n\n        return cells[:expected_columns]\n\n    def extract_tables_from_message(self, message: str) -> List[List[List[str]]]:\n        \"\"\"ä»æ¶ˆæ¯ä¸­æå–è¡¨æ ¼ï¼Œæ­£ç¡®å¤„ç†å•å…ƒæ ¼å†…çš„|å­—ç¬¦\"\"\"\n        rows = message.split(\"\\n\")\n        tables = []\n        current_table = []\n        expected_columns = None\n\n        for row_idx, row in enumerate(rows):\n            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ ‡è®°ç¬¦ |\n            if \"|\" in row:\n                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºåˆ†éš”è¡Œ\n                temp_cells = [cell.strip() for cell in row.strip(\"|\").split(\"|\")]\n                if temp_cells and self.is_separator_row(temp_cells):\n                    self.log(f\"è·³è¿‡åˆ†éš”è¡Œï¼šç¬¬{row_idx+1}è¡Œ\")\n                    # åˆ†éš”è¡Œå¯ä»¥å¸®åŠ©ç¡®å®šåˆ—æ•°\n                    if expected_columns is None:\n                        expected_columns = len([cell for cell in temp_cells if cell.strip()])\n                        self.log(f\"ä»åˆ†éš”è¡Œç¡®å®šåˆ—æ•°ï¼š{expected_columns}\")\n                    continue\n\n                # è§£æè¡¨æ ¼è¡Œ\n                if expected_columns is None:\n                    # ç¬¬ä¸€è¡Œï¼Œç¡®å®šåˆ—æ•°\n                    cells = self.parse_table_row(row)\n                    expected_columns = len(cells)\n                    self.log(f\"ä»ç¬¬ä¸€è¡Œç¡®å®šåˆ—æ•°ï¼š{expected_columns}\")\n                else:\n                    # åç»­è¡Œï¼Œä½¿ç”¨å·²çŸ¥åˆ—æ•°\n                    cells = self.parse_table_row(row, expected_columns)\n\n                # éªŒè¯è¡Œçš„æœ‰æ•ˆæ€§\n                if len(cells) < 2:\n                    self.log(f\"è·³è¿‡æ— æ•ˆè¡Œï¼ˆåˆ—æ•°<2ï¼‰ï¼šç¬¬{row_idx+1}è¡Œ\")\n                    continue\n\n                cleaned_cells = [self.clean_cell_content(cell) for cell in cells]\n                current_table.append(cleaned_cells)\n                self.log(f\"æå–è¡Œ{row_idx+1}ï¼š{len(cleaned_cells)}åˆ— - {cleaned_cells[:2]}...\")\n\n            elif current_table:\n                # é‡åˆ°éè¡¨æ ¼è¡Œï¼Œå½“å‰è¡¨æ ¼ç»“æŸ\n                tables.append(current_table)\n                self.log(f\"è¡¨æ ¼ç»“æŸï¼Œå…±{len(current_table)}è¡Œ\")\n                current_table = []\n                expected_columns = None\n\n        # å¤„ç†æœ€åä¸€ä¸ªè¡¨æ ¼\n        if current_table:\n            tables.append(current_table)\n            self.log(f\"æœ€åè¡¨æ ¼ç»“æŸï¼Œå…±{len(current_table)}è¡Œ\")\n\n        return tables\n\n    def extract_to_data(self) -> Data:\n        \"\"\"æå–è¡¨æ ¼å¹¶è½¬æ¢ä¸ºDataå¯¹è±¡ï¼Œæ”¯æŒExcel/CSVæ ¼å¼\"\"\"\n        try:\n            input_text = self.input_text\n\n            if not input_text:\n                self.log(\"é”™è¯¯ï¼šæœªæä¾›è¾“å…¥æ–‡æœ¬\")\n                # è¿”å›ç©ºçš„è¡¨æ ¼æ•°æ®\n                return Data(data={})\n\n            tables = self.extract_tables_from_message(input_text)\n\n            if not tables:\n                self.log(\"è¾“å…¥æ–‡æœ¬ä¸­æœªå‘ç°è¡¨æ ¼\")\n                return Data(data={})\n\n            # å¤„ç†æœ€åä¸€ä¸ªè¡¨æ ¼\n            table = tables[-1]\n            if len(table) < 2:\n                self.log(\"è¡¨æ ¼æ•°æ®ä¸è¶³\")\n                return Data(data={})\n\n            headers = table[0]\n            data_rows = table[1:]\n\n            # åˆ›å»ºDataFrame - ç°åœ¨åˆ—æ•°åº”è¯¥æ˜¯åŒ¹é…çš„\n            df = pd.DataFrame(data_rows, columns=headers)\n\n            # å°è¯•è½¬æ¢æ•°å€¼åˆ—\n            for col in df.columns:\n                df[col] = pd.to_numeric(df[col], errors=\"ignore\")\n\n            self.log(f\"æˆåŠŸå¤„ç†è¡¨æ ¼ï¼Œç”Ÿæˆ {len(df)} è¡Œæ•°æ®\")\n            self.log(f\"åˆ—åï¼š{headers}\")\n            self.log(f\"DataFrameå½¢çŠ¶ï¼š{df.shape}\")\n\n            # ä½¿ç”¨å­—å…¸æ ¼å¼ï¼šæ¯ä¸ªåˆ—åå¯¹åº”ä¸€ä¸ªå€¼åˆ—è¡¨\n            # è¿™ç§æ ¼å¼æ—¢æ»¡è¶³Data schemaçš„å­—å…¸è¦æ±‚ï¼Œåˆèƒ½è¢«pd.DataFrame()å¤„ç†\n            table_data = df.to_dict('list')  # {'col1': [val1, val2, ...], 'col2': [...]}\n\n            return Data(data=table_data)\n\n        except Exception as e:\n            error_msg = f\"å¤„ç†è¡¨æ ¼æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\"\n            self.log(error_msg)\n            return Data(data={})"
              },
              "input_text": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Text containing markdown tables (from AI output)",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_text",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TableToExcel"
        },
        "dragging": false,
        "id": "TableToExcel-RWezt",
        "measured": {
          "height": 219,
          "width": 320
        },
        "position": {
          "x": 1431.4059834052866,
          "y": 579.8002994931433
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "SaveToFile-RHxa4",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Save data to a local file in the selected format.",
            "display_name": "Save File",
            "documentation": "https://docs.langflow.org/components-processing#save-file",
            "edited": false,
            "field_order": [
              "input",
              "file_name",
              "file_format"
            ],
            "frozen": false,
            "icon": "save",
            "legacy": false,
            "lf_version": "1.6.4",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "File Path",
                "group_outputs": false,
                "method": "save_to_file",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nfrom collections.abc import AsyncIterator, Iterator\nfrom pathlib import Path\n\nimport orjson\nimport pandas as pd\nfrom fastapi import UploadFile\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.api.v2.files import upload_user_file\nfrom langflow.custom import Component\nfrom langflow.io import DropdownInput, HandleInput, StrInput\nfrom langflow.schema import Data, DataFrame, Message\nfrom langflow.services.database.models.user.crud import get_user_by_id\nfrom langflow.services.deps import get_settings_service, get_storage_service, session_scope\nfrom langflow.template.field.base import Output\n\n\nclass SaveToFileComponent(Component):\n    display_name = \"Save File\"\n    description = \"Save data to a local file in the selected format.\"\n    documentation: str = \"https://docs.langflow.org/components-processing#save-file\"\n    icon = \"save\"\n    name = \"SaveToFile\"\n\n    # File format options for different types\n    DATA_FORMAT_CHOICES = [\"csv\", \"excel\", \"json\", \"markdown\"]\n    MESSAGE_FORMAT_CHOICES = [\"txt\", \"json\", \"markdown\"]\n\n    inputs = [\n        HandleInput(\n            name=\"input\",\n            display_name=\"Input\",\n            info=\"The input to save.\",\n            dynamic=True,\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        StrInput(\n            name=\"file_name\",\n            display_name=\"File Name\",\n            info=\"Name file will be saved as (without extension).\",\n            required=True,\n        ),\n        DropdownInput(\n            name=\"file_format\",\n            display_name=\"File Format\",\n            options=list(dict.fromkeys(DATA_FORMAT_CHOICES + MESSAGE_FORMAT_CHOICES)),\n            info=\"Select the file format to save the input. If not provided, the default format will be used.\",\n            value=\"\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [Output(display_name=\"File Path\", name=\"message\", method=\"save_to_file\")]\n\n    async def save_to_file(self) -> Message:\n        \"\"\"Save the input to a file and upload it, returning a confirmation message.\"\"\"\n        # Validate inputs\n        if not self.file_name:\n            msg = \"File name must be provided.\"\n            raise ValueError(msg)\n        if not self._get_input_type():\n            msg = \"Input type is not set.\"\n            raise ValueError(msg)\n\n        # Validate file format based on input type\n        file_format = self.file_format or self._get_default_format()\n        allowed_formats = (\n            self.MESSAGE_FORMAT_CHOICES if self._get_input_type() == \"Message\" else self.DATA_FORMAT_CHOICES\n        )\n        if file_format not in allowed_formats:\n            msg = f\"Invalid file format '{file_format}' for {self._get_input_type()}. Allowed: {allowed_formats}\"\n            raise ValueError(msg)\n\n        # Prepare file path\n        file_path = Path(self.file_name).expanduser()\n        if not file_path.parent.exists():\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n        file_path = self._adjust_file_path_with_format(file_path, file_format)\n\n        # Save the input to file based on type\n        if self._get_input_type() == \"DataFrame\":\n            confirmation = self._save_dataframe(self.input, file_path, file_format)\n        elif self._get_input_type() == \"Data\":\n            confirmation = self._save_data(self.input, file_path, file_format)\n        elif self._get_input_type() == \"Message\":\n            confirmation = await self._save_message(self.input, file_path, file_format)\n        else:\n            msg = f\"Unsupported input type: {self._get_input_type()}\"\n            raise ValueError(msg)\n\n        # Upload the saved file\n        await self._upload_file(file_path)\n\n        # Return the final file path and confirmation message\n        final_path = Path.cwd() / file_path if not file_path.is_absolute() else file_path\n\n        return Message(text=f\"{confirmation} at {final_path}\")\n\n    def _get_input_type(self) -> str:\n        \"\"\"Determine the input type based on the provided input.\"\"\"\n        # Use exact type checking (type() is) instead of isinstance() to avoid inheritance issues.\n        # Since Message inherits from Data, isinstance(message, Data) would return True for Message objects,\n        # causing Message inputs to be incorrectly identified as Data type.\n        if type(self.input) is DataFrame:\n            return \"DataFrame\"\n        if type(self.input) is Message:\n            return \"Message\"\n        if type(self.input) is Data:\n            return \"Data\"\n        msg = f\"Unsupported input type: {type(self.input)}\"\n        raise ValueError(msg)\n\n    def _get_default_format(self) -> str:\n        \"\"\"Return the default file format based on input type.\"\"\"\n        if self._get_input_type() == \"DataFrame\":\n            return \"csv\"\n        if self._get_input_type() == \"Data\":\n            return \"json\"\n        if self._get_input_type() == \"Message\":\n            return \"json\"\n        return \"json\"  # Fallback\n\n    def _adjust_file_path_with_format(self, path: Path, fmt: str) -> Path:\n        \"\"\"Adjust the file path to include the correct extension.\"\"\"\n        file_extension = path.suffix.lower().lstrip(\".\")\n        if fmt == \"excel\":\n            return Path(f\"{path}.xlsx\").expanduser() if file_extension not in [\"xlsx\", \"xls\"] else path\n        return Path(f\"{path}.{fmt}\").expanduser() if file_extension != fmt else path\n\n    async def _upload_file(self, file_path: Path) -> None:\n        \"\"\"Upload the saved file using the upload_user_file service.\"\"\"\n        if not file_path.exists():\n            msg = f\"File not found: {file_path}\"\n            raise FileNotFoundError(msg)\n\n        with file_path.open(\"rb\") as f:\n            async with session_scope() as db:\n                if not self.user_id:\n                    msg = \"User ID is required for file saving.\"\n                    raise ValueError(msg)\n                current_user = await get_user_by_id(db, self.user_id)\n\n                await upload_user_file(\n                    file=UploadFile(filename=file_path.name, file=f, size=file_path.stat().st_size),\n                    session=db,\n                    current_user=current_user,\n                    storage_service=get_storage_service(),\n                    settings_service=get_settings_service(),\n                )\n\n    def _save_dataframe(self, dataframe: DataFrame, path: Path, fmt: str) -> str:\n        \"\"\"Save a DataFrame to the specified file format.\"\"\"\n        if fmt == \"csv\":\n            dataframe.to_csv(path, index=False)\n        elif fmt == \"excel\":\n            dataframe.to_excel(path, index=False, engine=\"openpyxl\")\n        elif fmt == \"json\":\n            dataframe.to_json(path, orient=\"records\", indent=2)\n        elif fmt == \"markdown\":\n            path.write_text(dataframe.to_markdown(index=False), encoding=\"utf-8\")\n        else:\n            msg = f\"Unsupported DataFrame format: {fmt}\"\n            raise ValueError(msg)\n        return f\"DataFrame saved successfully as '{path}'\"\n\n    def _save_data(self, data: Data, path: Path, fmt: str) -> str:\n        \"\"\"Save a Data object to the specified file format.\"\"\"\n        if fmt == \"csv\":\n            pd.DataFrame(data.data).to_csv(path, index=False)\n        elif fmt == \"excel\":\n            pd.DataFrame(data.data).to_excel(path, index=False, engine=\"openpyxl\")\n        elif fmt == \"json\":\n            path.write_text(\n                orjson.dumps(jsonable_encoder(data.data), option=orjson.OPT_INDENT_2).decode(\"utf-8\"), encoding=\"utf-8\"\n            )\n        elif fmt == \"markdown\":\n            path.write_text(pd.DataFrame(data.data).to_markdown(index=False), encoding=\"utf-8\")\n        else:\n            msg = f\"Unsupported Data format: {fmt}\"\n            raise ValueError(msg)\n        return f\"Data saved successfully as '{path}'\"\n\n    async def _save_message(self, message: Message, path: Path, fmt: str) -> str:\n        \"\"\"Save a Message to the specified file format, handling async iterators.\"\"\"\n        content = \"\"\n        if message.text is None:\n            content = \"\"\n        elif isinstance(message.text, AsyncIterator):\n            async for item in message.text:\n                content += str(item) + \" \"\n            content = content.strip()\n        elif isinstance(message.text, Iterator):\n            content = \" \".join(str(item) for item in message.text)\n        else:\n            content = str(message.text)\n\n        if fmt == \"txt\":\n            path.write_text(content, encoding=\"utf-8\")\n        elif fmt == \"json\":\n            path.write_text(json.dumps({\"message\": content}, indent=2), encoding=\"utf-8\")\n        elif fmt == \"markdown\":\n            path.write_text(f\"**Message:**\\n\\n{content}\", encoding=\"utf-8\")\n        else:\n            msg = f\"Unsupported Message format: {fmt}\"\n            raise ValueError(msg)\n        return f\"Message saved successfully as '{path}'\"\n"
              },
              "file_format": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "File Format",
                "dynamic": false,
                "external_options": {},
                "info": "Select the file format to save the input. If not provided, the default format will be used.",
                "name": "file_format",
                "options": [
                  "csv",
                  "excel",
                  "json",
                  "markdown",
                  "txt"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "excel"
              },
              "file_name": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "File Name",
                "dynamic": false,
                "info": "Name file will be saved as (without extension).",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "file_name",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "input": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": true,
                "info": "The input to save.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "SaveToFile"
        },
        "dragging": false,
        "id": "SaveToFile-RHxa4",
        "measured": {
          "height": 247,
          "width": 320
        },
        "position": {
          "x": 1945.385496293818,
          "y": 678.5238960313458
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "OpenAIModel-eoUhg",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generates text using OpenAI LLMs.",
            "display_name": "OpenAI",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout"
            ],
            "frozen": false,
            "icon": "OpenAI",
            "last_updated": "2025-11-25T10:31:17.259Z",
            "legacy": false,
            "lf_version": "1.6.4",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import (\n    OPENAI_CHAT_MODEL_NAMES,\n    OPENAI_REASONING_MODEL_NAMES,\n)\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import BoolInput, DictInput, DropdownInput, IntInput, SecretStrInput, SliderInput, StrInput\nfrom langflow.logging import logger\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_CHAT_MODEL_NAMES[0],\n            combobox=True,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n            required=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            show=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        IntInput(\n            name=\"max_retries\",\n            display_name=\"Max Retries\",\n            info=\"The maximum number of retries to make when generating.\",\n            advanced=True,\n            value=5,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"The timeout for requests to OpenAI completion API.\",\n            advanced=True,\n            value=700,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        logger.debug(f\"Executing request with model: {self.model_name}\")\n        parameters = {\n            \"api_key\": SecretStr(self.api_key).get_secret_value() if self.api_key else None,\n            \"model_name\": self.model_name,\n            \"max_tokens\": self.max_tokens or None,\n            \"model_kwargs\": self.model_kwargs or {},\n            \"base_url\": self.openai_api_base or \"https://api.openai.com/v1\",\n            \"max_retries\": self.max_retries,\n            \"timeout\": self.timeout,\n        }\n\n        # TODO: Revisit if/once parameters are supported for reasoning models\n        unsupported_params_for_reasoning_models = [\"temperature\", \"seed\"]\n\n        if self.model_name not in OPENAI_REASONING_MODEL_NAMES:\n            parameters[\"temperature\"] = self.temperature if self.temperature is not None else 0.1\n            parameters[\"seed\"] = self.seed\n        else:\n            params_str = \", \".join(unsupported_params_for_reasoning_models)\n            logger.debug(f\"{self.model_name} is a reasoning model, {params_str} are not configurable. Ignoring.\")\n\n        output = ChatOpenAI(**parameters)\n        if self.json_mode:\n            output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_REASONING_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = False\n            build_config[\"seed\"][\"show\"] = False\n            # Hide system_message for o1 models - currently unsupported\n            if field_value.startswith(\"o1\") and \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = False\n        if field_name in {\"base_url\", \"model_name\", \"api_key\"} and field_value in OPENAI_CHAT_MODEL_NAMES:\n            build_config[\"temperature\"][\"show\"] = True\n            build_config[\"seed\"][\"show\"] = True\n            if \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = True\n        return build_config\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "json_mode": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "JSON Mode",
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "list": false,
                "list_add_label": "Add More",
                "name": "json_mode",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "max_retries": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Retries",
                "dynamic": false,
                "info": "The maximum number of retries to make when generating.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_retries",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 5
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 0
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "Additional keyword arguments to pass to the model.",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": true,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "claude-sonnet-4-20250514"
              },
              "openai_api_base": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_base",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": false,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "type": "slider",
                "value": 0.66
              },
              "timeout": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "The timeout for requests to OpenAI completion API.",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 700
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "OpenAIModel"
        },
        "dragging": false,
        "id": "OpenAIModel-eoUhg",
        "measured": {
          "height": 701,
          "width": 320
        },
        "position": {
          "x": 853.4185095373903,
          "y": 592.7872791260453
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -206.03734257723636,
      "y": -434.22065817997964,
      "zoom": 1.0523760068551393
    }
  },
  "description": "AI automated test case generation based on requirements documentation",
  "endpoint_name": null,
  "id": "d8e05dcd-6454-42f4-b56d-6e43dced0a75",
  "is_component": false,
  "last_tested_version": "1.6.4",
  "name": "Test Case Generation Flow ",
  "tags": [
    "chatbots"
  ]
}